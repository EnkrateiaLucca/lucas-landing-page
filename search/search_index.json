{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sup Folks!","text":"<p>I'm an AI Engineer, and AI instructor at O'Reilly Media, I like writing about building cool tools.  A few years ago, after spending some time as a research assistant at the Champalimaud Foundation, I pivoted to industry and started working as an AI engineer, and now I help people develop AI models, tools, and all sorts of fun stuff.</p> <p>From time to time I also do some neat workshops about stuff I find interesting in AI.</p> <p>Subscribe to my Newsletter Subscribe to my YouTube</p>"},{"location":"blog/","title":"Writing","text":"<p>I write about</p> <ul> <li>\ud83d\udd27 Building AI Tools</li> <li>\ud83e\udd16 Using AI Tools</li> </ul> <p>Subscribe to my Newsletter Subscribe to my YouTube</p>"},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/","title":"Quick Tips on Using LLMs Effectively","text":""},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/#quick-tips-on-using-llms-effectively","title":"Quick Tips on Using LLMs Effectively","text":"<p>All right, this is going to be my own attempt at compiling some fun examples of how to prompt LLM models effectively to do useful stuff.</p>"},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/#prompt-iteratively","title":"Prompt Iteratively","text":"<p>I think it was Jeremy Howard who coined this term 'Dialog Engineering' where you build and engineer things through talking to an LLM in small steps.</p> <p>This could not be more true. Prompt atomically, for example: instead of asking a model to build a 'quiz app' maybe ask the model to:</p> <ol> <li>\"Design a basic data structure for quiz questions and answers\"</li> <li>\"Create a function to load and parse quiz questions from a JSON file\" </li> <li>\"Build a simple command-line interface to display questions and accept user input\"</li> <li>\"Add scoring logic to track correct/incorrect answers\"</li> <li>\"Implement a way to save quiz results and show final score\"</li> </ol> <p>Something like that where you prompt in small pieces and for each you supervise the result and incrementally grow into the final result you were looking for.</p> <p>This is a perfect segway to my next tip.</p>"},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/#use-llms-as-assistants-not-as-replacements","title":"Use LLMs as assistants not as replacements","text":"<p>Don't treat whatever is generated with an AI as the final all might output. Treat everything you get from it critically, which I know can sound a bit contradictory given the nature of why we are using LLMs right? We are using it so we don't have to do the work. However, this approach can only lead to hours of mindless debugging and absolute dread. </p> <p>Instead, treat the model like the great Simon Willison puts it in this youtube video where he mentions you should treat them as \"smart interns\" that \"read through all the documentation\" and can help 24/7.</p> <p>I think you should use them as supporting like tools to support support the decisions that you're making... - Simon Willison </p>"},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/#ask-for-multiple-options","title":"Ask for Multiple Options","text":"<p>Specially for tought questions, don't ask the models for one answer, ask for many and pick the ones that looks best.</p>"},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/#use-it-to-explore-and-not-just-for-quick-answers","title":"Use it to Explore and Not Just for Quick Answers","text":"<p>Do side projects with these tools and explore what they can do instead of  relying on them just as a google search replacement.</p>"},{"location":"blog/2025/02/02/quick-tips-on-using-llms-effectively/#explore-and-experiment","title":"Explore and Experiment","text":"<p>When working with AI tools, it's important to approach them with a spirit of exploration and experimentation rather than just using them for quick answers. Here are some key ways to do this:</p> <p>Challenge yourself to do complete projects using AI tools. As one developer put it: \"If you can afford to do a side project with these tools and like set yourself a challenge to write every line of code with these tools, I think that's a great thing you can do.\"</p>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/","title":"Patterns for LLM Usage","text":""},{"location":"blog/2025/02/02/patterns-for-llm-usage/#patterns-for-effective-usage-of-llms","title":"Patterns for Effective Usage of LLMs","text":""},{"location":"blog/2025/02/02/patterns-for-llm-usage/#code-generation","title":"Code Generation","text":"<p>Paste code + error ask it to debug</p> <p>Copy paste from ChatGPT/Claude/Gemini/Llama3 and also paste in error + original code to get better answer.</p> <p>Automate Cleaning AI Output</p> <p>Setup a quick tool to clean up the Python code generated (I use an alias <code>clean-python</code>)</p> <p>Use Standalone Scripts with AI + uv</p> <p>Generate Python standalone scripts by using Claude+Projects with custom descriptions and the uv package manager</p> <p>Leverage Context</p> <p>For recent coding frameworks use the documentation as json/markdown files as context for something like Claude/ChatGPT projects. Show LLM how to call an API (in the prompt) then ask it to create something with that api.</p>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/#general-usage","title":"General Usage","text":"<p>Let it see your screen</p> <p>Use tools like Gemini 2.0 with streaming in realtime in Google AI Studio so the AI can see your screen to help you navigate new software and answer app specific questions in context.</p> <p>Save your AI Mistakes</p> <p>When AI makes a mistake save it for later as your own personal benchmark</p> <p>Build Micro-AI-Powered Data Transformation Pipelines</p> <p>Build app with AI that takes in data with a certain structure and outputs  desirable output, format, etc.... then make a prompt template that  produces the data into the format acceptable  by that app (done).  </p> <p>Conceptual Knowledge Prompting</p> <p>When creating knowledge management prompts (like for Anki cards), structure them around these 5 key dimensions: - Attributes &amp; tendencies - Similarities and differences - Parts &amp; Wholes - Causes &amp; Effects - Significance &amp; Implications</p> <p>This helps move beyond simple memorization towards deeper encoding of knowledge.</p> <p>Background AI Assistance</p> <p>Run lighter models (Tier 3) in the background to provide gentle guidance without disrupting workflow. For example: - Watching note creation to suggest knowledge structuring patterns - Using SuperWhisper to transform free-form thinking into structured content - Converting audio brainstorming into written documentation</p>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/#patterns-for-prompt-templates","title":"Patterns for Prompt Templates","text":"<p>Informed transformation </p> <pre><code>Given this {{ context }}. Do {{ action }} to this {{ content }}.\n</code></pre> <p>The OUTPUT ONLY Pattern</p> <p>Prime model at the very end to: </p> <p><code>(...previous context...) OUTPUT ONLY {{ desired output }}</code></p> <p>Use Meta Prompts</p> <p>Use prompts for prompts! Create a prompt that uses a model to  generate multiple prompts that address all the parts of your task.</p> <p></p> <p>For example:</p> <pre><code>I need to create a series of prompts to help me analyze customer feedback data. \nPlease generate 3 prompts for an LLM model to help me:\n\n1. Extract key themes and sentiment\n2. Identify urgent issues needing attention\n3. Generate actionable insights for product improvements\n\nFor each prompt you generate, explain its purpose and expected output format.\n\nOUTPUT ONLY the prompts and their explanations, formatted as such:\nPURPOSE: &lt;purpose of prompt&gt;\nInstruction: &lt;main instruction&gt;\nOUTPUT FORMAT: &lt;desired output format&gt;\n</code></pre> <p>You can also ask the model to break down the problem itself given some  initial goal or intention and then for each sub-task ask for a solution:</p>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/#which-models-to-use-when","title":"Which Models to Use When","text":""},{"location":"blog/2025/02/02/patterns-for-llm-usage/#model-tiers","title":"Model Tiers","text":""},{"location":"blog/2025/02/02/patterns-for-llm-usage/#tier-1-high-intelligence-slow-expensive","title":"Tier 1 (High Intelligence, Slow, Expensive)","text":"<ul> <li>For complex, nuanced tasks</li> <li>Examples: DeepSeek, O1.  </li> </ul>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/#tier-2-balanced","title":"Tier 2 (Balanced)","text":"<ul> <li>It is your daily driver for most tasks \u2013 code, emails, general queries</li> <li>Examples: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.3.</li> </ul>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/#tier-3-cheap-fast","title":"Tier 3 (Cheap, Fast)","text":"<ul> <li>For bulk, everyday tasks</li> <li>Examples: GPT-4o-mini, Gemini Flash, Llama 3.\u2153.2.</li> <li>Enables AI usage in \"every nook and cranny\"</li> </ul>"},{"location":"blog/2025/02/02/patterns-for-llm-usage/#workflow-example","title":"Workflow Example","text":"<ol> <li>Use Tier 3 to process large documents quickly and cheaply</li> <li>Use Tier 2 to refine and apply structured outputs</li> <li>Use Tier 1 for final critical reasoning or complex synthesis</li> </ol>"},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/","title":"An Intuitive Introduction to LangGraph","text":""},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#an-intuitive-introduction-to-langgraph","title":"An Intuitive Introduction to LangGraph","text":""},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#routing-stuff","title":"Routing Stuff","text":"<p>I'm working on my LangGraph course and I kind of wanted to do a little recap of how I see this framework in the big scheme of things.</p> <p>When working with LLMs, one common pattern that exists is to route stuff, meaning you send some input to an LLM and the output of that gets routed to either another LLM as described in this article by Anthropic about common patterns for working with LLMs or even to some other functionality like you might use some Python function to clean the output of that LLM.  </p> <p></p> <p>This is great because the LLM has some type of understanding of what is going on even though we still don't really understand the nature of this understanding, however it is good enough that we can actually use it to make some processing pipeline a bit leaner by having LLMs make certain decisions within some narrow scopes of a workflow.</p>"},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#the-abstraction-challenge","title":"The Abstraction Challenge","text":"<p>Now, if I am writing up a system that can perform complex tasks, </p> What do I mean by complex? <p>Integration of a diverse set of components that do all sorts of different things like performing actions, calling apis, processing massive amounts of diverse data like text and images, and so on.</p> <p>it is a bit problematic to implement it just as simple calls to apis like OpenAI or Anthropic's, because we want that system to be pragmatically improvable (robust, consistent). </p> <p>What I mean is that despite wanting the flexibility brought by LLMs, we also want the controllability offered by writing software that does stuff deterministically and can be systematically improved over time.</p> <p>So that begs the question of how we create useful abstractions around the capabilities of Large Language Models? </p> <p>To understand that, we need to understand what we are trying to abstract, for that, let's take the most common pattern in this emerging field of Agents that is growing quite a bit this year: React Agent.</p> <p></p> <p>What is there to abstract?</p> <p>Well, a bunch of stuff, if want to be able to develop a system aroud these things we need to be able to abtract things like:</p> <ol> <li>Messaging between user and LLM apis</li> <li>Calling LLM apis</li> <li>Integrating tools into LLMs as functions that call external APIs</li> </ol> <p>and much more (see diagram below).</p> <p></p> <p>Not only that, but we also want to be able to track and monitor these parts so that when problems occured we can investigate what happen and debug our system across all of its parts.</p>"},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#langchain","title":"LangChain","text":"<p>LangChain came into the scene as a framework that allowed you to put all these different parts that are common when building LLM apps, into the same 'system'. </p> <p>In LangChain everything is a runnable, which means, things like calling an LLM, calling a tool that performs some action, the prompt that you send to the model, and many other things like that become all components in this runnable interface that can be organized via the usage of a declarative language called LCEL (LangChain Expression Language).</p> <p>The idea is that you can create <code>chains</code> which in LangChain are building blocks made out of the component parts like <code>prompt template</code> <code>chat models</code> and many others, in order to create modular workflows that have swappable parts.</p> <p></p> <p>LangChain became extremely popular I think in part because they were the first to realize that there was much more value to be extracted from LLMs than just asking them for text and getting results back, building on top of super important papers that started to explore these additional functionalities like:</p> <ul> <li>Toolformer: Language Models Can Teach Themselves to Use Tools</li> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</li> <li>ReAct: Synergizing Reasoning and Acting in Language Models</li> </ul> <p>However, building this framework came at the cost of having to make choices for how the infrastructure around LLMs should look like, which is not a trivial problem because we are talking about a technology in its infancy, consider that ChatGPT was released in 2022!</p>"},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#langgraph","title":"LangGraph","text":"<p>Regardless of what the ultimate abstraction for LLMs look like, what we know right now is that there are common patterns for using it, which usually involve taking some input and passing it through a set of transformations, some of which are handled by LLMs (so not completely deterministic) and some are handled by normal code.</p> <p>What the LangChain folks realized is that most of what we do with LLMs could actually be handled by treating the entire process as graphs with nodes and edges to integrate controllability into the system while mixing it with the flexible nature of LLMs.</p> <p>So LangGraph was born.</p> <p>The way I look at it, LangChain gives you the material to do the basics with LLMs like:</p> <ul> <li>Call LLM APIs</li> <li>Integrate Prompts</li> <li>Loading documents</li> <li>Tools, retrievers for rag systems</li> <li>....</li> </ul> <p>however, when it comes to putting together a system, connecting different things and managing all of that, the LCEL comes short of something simple and intuitive that can give you manageable complexity and controlability.</p> <p>So LangGraph shows up as a framework that can take in the standardization provided by operating on LangChain Components, and provide the graph building capabilities that are considerably more intuitive when compared to 'chaining runnables'.</p> <p></p> <p>In LangGraph you introduce cycles into these chains, in the form of 'controlled flows' or 'state machines':</p> <p>LangGraph is a way to create these state machines by specifying them as graphs.</p>"},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#what-do-we-get-with-graphs","title":"What Do we Get with Graphs?","text":""},{"location":"blog/2025/02/10/an-intuitive-introduction-to-langgraph/#states-nodes-edges","title":"States, Nodes &amp; Edges","text":"<p>In LangGraph the logic is to define some initial 'state' which is a data structure that will be updated throughout the execution of the graph.</p> <p>Let's take this diagram we showed of the simple react agent loop, where we have some input coming in to a model connected to some tools, and the model is going to go on a loop of calling said tools until a response is generated.</p> <p>For something like that we would need to define:</p> <ol> <li>The LLM to use</li> <li>The tools that model has access to</li> <li>Connect LLM to the tools (so the model can create the arguments for it)</li> </ol> <pre><code>from langchain_community.tools import TavilySearchResults \nfrom langchain_anthropic import ChatAnthropic\n\n# The LLM\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n# The tool (for web search)\nsearch_tool = TavilySearchResults()\n# Connect LLM to tool \nllm_with_tools = llm.bind_tools([search_tool])\n</code></pre> <p>Now, we need to take this core set of functionality we've built and inserted in a graph. To do so we need:</p> <ol> <li>To define a state</li> <li>Nodes (that perform the computations)</li> <li>Edges (that connect everything together)</li> </ol> <p>The state will define this updatable data structure that will change throughout the execution of the graph. The nodes will be the functions that perform the computations that happen throughout this graph like running LLM calls, using the tools and so on.</p> <p>The edges will join all the nodes together defining the direction of this graph.</p> <p>We start by defining the nodes as simple python functions:</p> <pre><code># The node where the LLM+tools is called\ndef llm_node(state: MessagesState):\n    # Gets the latest message in the messages list\n    response = llm_with_tools.invoke(state[\"messages\"])\n    print(\"response\")\n    print(response)\n    # returns a dictionary that corresponds to updating the state\n    # adding a the message from the model's response\n    return {\"messages\": [response]}\n\n# The node that performs the conditional logic that defines whether the\n# LLM will call a tool and return a final output to the user \ndef router_node(state: MessagesState):\n    if state[\"messages\"][-1].tool_calls:\n        # routes to a node called: \"tools\"\n        return \"tools\"\n\n    # if there is no action required, we end the loop\n    return END\n</code></pre> <p>NOw that we have the nodes as python functions, we can define our graph.  We start by defining the initial state as a <code>MessagesState</code> object, which means its a data structure that contains a default list of messages inside + the ability to add more messages to this list as the graph get's executed.</p> <pre><code>builder = StateGraph(MessagesState)\n</code></pre> <p>We integrate the nodes we've created earlier: <pre><code>builder.add_node(\"llm\", llm_node)\nbuilder.add_node(\"tools\", tool_node)\n</code></pre></p> <p>We set an entry point for the graph <pre><code># we set the entry point for the graph to be the llm\n# which means the user will send the input directly to the LLM\nbuilder.add_edge(START, \"llm\")\n</code></pre> Connect the llm to the function containing the conditional logic that routes information either to the user (END) or to the node containing the tools, and compile the graph.</p> <pre><code>builder.add_conditional_edges(\"llm\", router_node, [\"tools\", END])\n# Here we make it so that the output of the tools node has to go to the \"llm\"\nbuilder.add_edge(\"tools\", \"llm\")\n\ngraph = builder.compile()\n</code></pre> <p>Great! Now we can see how it looks!</p> <p><pre><code>try:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    print(\"Did not display graph\")\n    pass\n</code></pre> </p> <p>Looks pretty simple right? But now, we can actually have it look up information online and return written results!</p> <pre><code>graph.invoke(\n    {\n        \"messages\": [\"human\", \"Search online for current AI conferences happening in Silicon Valley right now.\"]\n    }\n)\n</code></pre> <p>There are many other core concepts in LangGraph such as:</p> <ul> <li>Persistence &amp; Memory</li> <li>Subgraphs</li> <li>Configuration</li> <li>Command</li> <li>....</li> </ul> <p>We'll leave that for another article! :)</p> <p>Cheers!</p> <p>Subscribe to my Newsletter</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/","title":"Using AI To Build Practice","text":""},{"location":"blog/2025/02/16/using-ai-to-build-practice/#using-ai-to-build-practice","title":"Using AI to Build Practice","text":"<p>So, these are a few examples on how one can use AI to do what I like to call building practice.</p> Building Practice <p>Building practice is about systematically tackling a desired skill like 3d printing, playing the piano and so on, through the process of building a system that involves a closed looop environment for improving particular aspects of that skill.</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#fetching-data-from-documents","title":"Fetching data from documents","text":"<p>Document-based learning can be enhanced by uploading technical PDFs and instructing the model to answer questions specifically from that content, with references to exact pages. This creates a focused learning loop for understanding specific software functions or technical concepts. The model can also help locate relevant information online through a search action that can potentially save a lot of time on manual searches across multiple URLs.</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#breakdown-pattern","title":"Breakdown pattern","text":"<p>One fundamental pattern is what I call the breakdown pattern where you feed the model your ultimate goal and ask it to break it down into smaller, more manageable tasks or projects. </p> <p>This creates a natural progression from simple to complex challenges, allowing you to build the necessary skills incrementally while maintaining an optimal balance of difficulty at each step.</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#suggestion-pattern","title":"Suggestion pattern","text":"<p>To complement this approach, the suggestion pattern helps maintain momentum in your learning journey. When you're unsure what to tackle next, you can describe your current skill level and completed projects to the model, which can then suggest appropriate next steps. This eliminates friction in decision-making and keeps you moving forward in your skill development.</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#real-time-assistance","title":"Real-time assistance","text":"<p>Real-time assistance when learning a new software tool for example you can use something like Google AI Studio's real-time screen-sharing feature with Gemini to navigate complex software interfaces like AutoDesk Fusion 360. </p> <p>While not perfect, the model can provide helpful guidance and suggestions for navigating complicated user interfaces. There are some cool experimental features being built on top of this like this Gemini Cursor.</p> <p>There are also other interesting approaches related to this concept and the idea of computer use like the new tool from Microsoft: OmniParser 2.</p> <p>More gemini-based apps can be found here: Awesome-Gemini-Apps.</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#brainstorming","title":"Brainstorming","text":"<p>Brainstorming with AI becomes more effective when you carefully craft your prompts to receive specific, targeted interactive feedback. Rather than accepting vague advice from a generic conversation, you can structure the interaction to get precise, actionable information in your preferred format. For example, when practicing writing you might want the model to be more tuned to a certain style of writing like technical, or screenwriting and so on.</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#planning-learning-goals","title":"Planning learning goals","text":"<p>AI can also assist with planning by helping organize tasks around existing commitments and limitations. This extends beyond skill development to general productivity, allowing you to optimize your learning schedule and maintain steady progress.</p> <p>You can prompt the model to breakdown a specific goal with a certain deadline into a set of tasks that should be executed within some pre-defined schedule (that could also be given by the model).</p>"},{"location":"blog/2025/02/16/using-ai-to-build-practice/#maintaining-flow","title":"Maintaining flow","text":"<p>The overarching principle in using AI for skill development is maintaining flow state. It's crucial to avoid treating AI as a gimmick and instead craft specific, ultra-specialized interactions that provide exactly what you need. The goal isn't to replace your cognitive effort but to facilitate it. </p> <p>By developing small-scale closed learning loops with clear feedback mechanisms, you create an environment where AI enhances rather than substitutes for your learning process.</p> <p>This focused approach ensures that your interaction with AI directly supports your skill development while keeping you actively engaged in the learning process.</p>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/","title":"An Audio-based AI-co-writing workflow","text":""},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#an-audio-based-ai-co-writing-workflow","title":"An Audio-based AI-co-writing workflow","text":""},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#step-1-download-superwhisper-for-audio-transcription","title":"Step 1 - Download Superwhisper for Audio Transcription:","text":"<ul> <li>For IOS Users</li> </ul> <p>Superwhisper allows users to dictate thoughts and ideas, transforming them into structured text. This hands-free approach enables brainstorming sessions during activities like walking or commuting.  </p>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#step-2-use-chatgpt-for-idea-expansion-and-structuring","title":"Step 2 - Use ChatGPT for Idea Expansion and Structuring:","text":"<ul> <li>Download the ChatGPT app on the APP Store</li> </ul> <p>ChatGPT assists in organizing, expanding, and refining transcribed ideas. With a ChatGPT Plus subscription, users gain access to advanced features, enhancing the creative writing process.</p>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#step-3-integrate-transcriptions-into-chatgpt-for-refinement","title":"Step 3 - Integrate Transcriptions into ChatGPT for Refinement","text":"<ul> <li>Import the transcribed content from Superwhisper into ChatGPT.</li> <li>Use ChatGPT to brainstorm, reframe, and expand upon the initial ideas, structuring them into coherent narratives or chapters.</li> </ul>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#step-4-review-drafts-during-activities","title":"Step 4 - Review Drafts During Activities","text":"<ul> <li>Convert drafts into audio formats to listen during walks or other activities.</li> <li>You can do that with apps like speechify</li> <li>Note down feedback, ideas, or areas of improvement during these sessions (you can do it all with audio using super whisper or recording on your phone using the Iphone default voice memo app or something similar)</li> </ul>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#step-5-incorporate-feedback-into-revisions","title":"Step 5 - Incorporate Feedback into Revisions","text":"<ul> <li>Input the gathered feedback into ChatGPT to refine and enhance the drafts.</li> <li>Repeat this iterative process until the desired quality is achieved.</li> <li>At this stage compile insights as bites into something like a notes app or doc.</li> </ul>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#step-6-finalize-and-share","title":"Step 6 - Finalize and Share","text":"<ul> <li>Once satisfied, compile the chapters or sections into a complete manuscript.</li> <li>Share the final version with peers, editors, or publish as desired.</li> </ul>"},{"location":"blog/2025/03/08/an-audio-based-ai-co-writing-workflow/#example-walkthrough","title":"Example Walkthrough","text":"<ol> <li>Open SuperWhisper on your phone (step 3)</li> <li>Dictate your thoughts about what you want to write about or expand (make these dictations small less than a minute) (step 3)</li> <li>Copy paste that into ChatGPT with a Prompt that gives it the context of what you want to do for example you can prompt ChatGPT with this: (step 3) <pre><code>I'm going to brainstorm plot ideas for a space opera novel. Please take everything I say, focusing on key characters, main conflict, and potential locations. At the end, organize the notes into a simple bullet list with headings for Characters, Conflict, and Setting.\n\n{{PASTE YOUR AUDIO TRANSCRIBED FROM SUPERWHISPER HERE}}\n</code></pre></li> <li> <p>Take that output and reflect on it (perhaps during another walk?) and if you want to expand on it you can use the feature \"Advanced Voice Mode\" from ChatGPT directly on your phone to 'talk' to ChatGPT about those ideas. (step 3-4-5)</p> </li> <li> <p>Do this in a loop:</p> </li> <li>Record your thoughts with superwhisper</li> <li>Copy paste that into ChatGPT with some input for how you want to process those 'raw thoughts'<ol> <li>It can be like 'Structure this into bullet points'</li> <li>But also something like: 'Can you rephrase these ideas?'</li> <li>You can do whatever! Be creative</li> </ol> </li> <li>Take whatever output you get back as your starting point for the next part (always working in small gradual reflexive steps)</li> <li>Copy paste the main insights into something like a google doc or your notes app to keep track of insights as bites of thoughts that will be later compiled into something like a full blog post, chapter, etc...</li> <li>Iterate until you have a draft (that usually is a bunch of rich insightful notes inside ChatGPT or that you copy pasted from there into some notes app or doc)</li> </ol>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/","title":"AI Tools Life Thoughts 2025","text":""},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#ai-tools-life-travel-thoughts","title":"AI Tools, Life, Travel &amp; Thoughts","text":"<p>In this post I just want to talk a little bit about some of the stuff I've been doing, what I've been 'working on', some AI tools I've been playing around with and some other stuff.</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#ai-tools-cursor","title":"AI Tools - Cursor","text":"<p>This year I've been using Cursor more and more, I've been following in love with the process of finding new avenus of leveraging LLMs in new and exciting ways. On the one hand I carry a bit of that fear of never being able to code again without some AI hanging over me, on the other, building things through conversation can be quite exciting. </p> <p>Even as I write this post I'm thinking! \"Hey, I don't have a simple automation to create a references section for my article!\". Then, immediately I already think: \"Ah, never mind, I can just ask Cursor to generate that when I'm done writing.</p> <p>Cursor is cool, and these are some of the things I've been getting more interested lately and intend to explore:</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#1-cursor-rules-files","title":"1. Cursor rules files","text":"<p>These are like files you can easily create <code>.cursorrules</code> and then inside you describe how Cursor should behave within the scope of a project.</p> <p>The cool thing is that you can make it even better by creating a folder on your root like: <code>.cursor/rules</code>, and then inside you can write spefications per programming language or file type, its bananas.</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#2-cursor-agentic-mode","title":"2. Cursor Agentic Mode","text":"<p>It's been fun to watch Cursor building whatever I want by just asking, then see the whole process unwind in front of me as if I'm some sort of low budget god of silly little apps.</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#3-mcp-cursor","title":"3. MCP + Cursor","text":"<p>I'll be honest, this is one of those that I haven't fully explored yet, essentially MCP came out and took the AI engineering world by storm. I haven't studied it in full yet, but essentialy its a standardization of what previously was simply LLM + tool calling in the wild. Now we have something more like a standard to follow to connect LLMs to tools, resources and prompts, and the promise seems to be quite exciting.</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#ai-tools-aider-claude-code-building-rust-apps","title":"AI Tools - Aider, Claude-code, Building Rust apps","text":"<p>Another fun things I've been doing lately is attempting to replace doom scrolling with what I think I'll start calling 'doom app creating' which essentially means talking to aider via its really neat <code>/voice</code> mode and essentially just ask it to build whatever is in my mind.</p> <p>What fascinates me more about this approach is how \"un-creative\" I can be, which I know, sounds kind of self-deprecating, but it does piss me off, like I am living in the most creative-friendly era in history and all I can come up with is like 'a todo-list with some fun interactive animations'? God damn...Davinci would have been very disappointed. </p> <p>Nevertheless, some times I have my moments and I think I come up with ideas that could at least have some potential, if it wasn't for me abandoning them immediately after I build a first 'barely functional' version of it. Some of the highlights for this week built with either Cursor or aider were:</p> <ol> <li> <p>Silly todo-list with rust (yes I built a todo list leave me alone)</p> <p></p> </li> <li> <p>A 'barely working' clipboard manager</p> <p></p> </li> <li> <p>A completely useless markdown note taking app (again why? It is beyond me at this point)</p> <p></p> </li> <li> <p>The worst 'Focus App' ever (essentially a timer with a dot in the middle for you to 'focus') </p> </li> <li> <p>A template generator app to leverage llms to iterate on templates for very precise/structured command generation (this one I think had some future!)</p> <p></p> <p>The idea here I think was actually interesting, essentially today it is super easy to generate like big terminal commands with LLMs, and I wanted to have like a super sleak, easy to use tool that would give me a minimal interface allowing to iterate on a certain structured command like the one in the image above. However, I tend to leave these silly little projects behind...but at least I wrote this little piece on it so it won't be completely forgotten.</p> </li> <li> <p>App to highlight code on the specific parts to edit given a prompt (again I liked this one!).     Again with this one, even though obviously you get this type of functionality in any IDE these days, I wanted something easy that works well in the terminal and again, it was just a minute with Cursor to have a perfectly functional version! Also, I am really interested in like ways to leverage AI to visualize stuff better so this was actually the first app I made with that concept in mind. </p> </li> <li> <p>App that allows you to navigate highlighted portions of a text     The idea here was that, since now it's so easy and fast to use embeddings (even in the terminal!) to select parts of a large text given some input (like a question, or a semantic request like 'the tools used in this paper'), I wanted to have a simple and easy to use interface to navigate those portions of the text once the relevant sections were extracted. Still have some faith for this project....</p> </li> </ol> <p>There are so many more but I think those are enough to pain the picture of the mess that is my 'random-apps' folder. Some of them were built with <code>aider</code> some with the new <code>claude-code</code>, all of them were built kind of without strong intentions, just as a exercise on chaotic experimentation.</p> <p>Now, one last note I'll say on this topic is the revolutionary change on my workflow and day-to-day by integrating llms in the terminal using SImon Willison's LLM-CLI. I can now write commands like:</p> <p><pre><code>`for f in *.txt; do echo ${f%.txt}.pdf &gt;&gt; file_classification.txt; cat $f | llm 'Give this file a one sentence description' &gt;&gt; file_classification.txt\n</code></pre> Which for me, has been the most fun I've had with AI besides what I'm about to tell you in the end of this article ;) .</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#ai-tools-llm-htmljs-apps","title":"AI Tools - LLM + HTML/JS Apps = \u2764\ufe0f","text":"<p>One combination I've found to work really well for a lot of stuff is to combine a powerful code model like Claude-3.7-sonnet, and then ask it to generate an app in pure html/js combined with this idea of splitting the logic of the code itself from the logic of the data (something that a lot of developers would be like 'seriously? Now you see the value of that? Oh wow!' but hey! I'm new-ish at this ok?). </p> <p>What you get is the ability to make apps that have 0 dependencies, can run directly on your browser and just works, so its like...whatever app you use that is simple enough can probably be replaced by a prompt,, 15 minutes and a cup of coffee ;).</p> <p>My biggest sucess with this combination was to write this quiz app in a single <code>.html</code> file that I use to test myself on different subjects. </p> <p>To say it has a 'simple interface' it would be an understatement, its as plain as it gets: </p> <p></p> <p>I load the questions in the right format (almost always generated by AI ;)) and then I quiz myself on whatever. The biggest use I had was to prepare for a 'The Office' themes quiz night at a bar here in Lisbon (hey we got 2<sup>nd</sup> place ok! B) ).</p> <p></p> <p></p> <p></p> <p>I've made some upgrades to this app but I haven't pulled the trigger yet because I love the simplicity of this current version.</p> <p>What I love the most about this is to be able to think of something that could be small and simple, yet powerfullly useful and effective for my current needs and workflows, and then just make it, and then use it! How cool is that?</p> <p>I even made a course that I teach live at the O'Reilly platform where I talk about this subject and I generally use this quiz app example as one of the study cases. Here is the course if you're interested:</p> <ul> <li>Building Simple Web Apps with AI Tools</li> </ul> <p>There is a lot to develop for this course still but I am excited for the next iteration of it coming up this year.</p>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#some-personal-stuff","title":"Some Personal Stuff","text":"<p>Ok, to be quick, let's list a few things that I think were good this year:</p> <ol> <li>LangChain Retweeted my article about building LLM agents in 3 levels of complexity (it got over 30.3k views!)</li> <li>I hosted my very first in-person AI Tools Workshop! This first iteration was about tools for filmmaking. I could have done a better job with the structure, but overall people really liked it and I think it was packed with actionable tips and practically insightful ideas. </li> <li>I've been working on my new workshop that I intend to publicize soon about AI tools for studying, learning and research with a focus on actually helping students and researchers to get the most out of AI without having to necessarily off load all the cognitive load and the effort of something to the AI.</li> <li>I also had my first 2 potential AI clients this year which is super exciting, both were related to building agents so that's something I'll probably be focusing this year.</li> <li>I had even more courses approved at OReilly where I currently teach a variety of online courses about LLMs, Agents and so on. The list is below if you want to check them out:<ol> <li>Using AI Tools and Python to Automate Tasks</li> <li>Building AI Apps with Gemini 2.0</li> <li>Working with o1, DeepSeek, and Gemini 2.0 Reasoning Capabilities</li> </ol> </li> </ol> <p>I also have 2 other courses right now in the works that I hope will be live in the second half of this year.</p> <ol> <li> <p>Working remote has its perks, one of which is travelling whenever I want! This year I've been to a few places already:</p> <ol> <li>Netherlands</li> <li>London</li> <li>Ireland</li> <li>Italy (Venice, Verona, Milan)</li> <li>Switzerland (spent a day in Zurich! :))</li> </ol> <p> </p> </li> </ol>"},{"location":"blog/2025/03/22/ai-tools-life-thoughts-2025/#final-thoughts-on-working-in-ai","title":"Final Thoughts on Working in AI","text":"<p>It has been a bit surreal, over these past 2 years, how my life changed completely because of AI. Like if only I knew how right my decision was to leave Brazil and come to Europe to pursue it. </p> <p>I remember it like it was yesterday, my excitement when I first started working at the Champalimaud Foundation, considered to be a world class lab for contributions to artificial intelligence I felt like that experience was going to shape my upcoming years, like my life was about to radically change, and indeed it happened! Even tough I didn't pursue a PhD in the lab, having finished my masters there and then having worked there as a hired research assistant (working on an application of generative adversarial networks to neuroscience settings) really helped me understand the inner workings of AI, how to use it, and how to get the most out of it.</p> <p>Now, working as an AI engineer, freelancer, AI instructor, everything seems quite magical, like sometimes I have to pinch myself because it feels unreal that I get paid to do the kind of stuff I get paid to do.</p> <p>From now on I'll be focusing more on producing higher technical quality content on Youtube, pursue more clients as a freelancing AI engineer, and continue to produce courses with OReilly as well as on my own, I can't wait for what's next :).</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/","title":"Reasoning Models, When to Use Them? What Are They Good for?","text":""},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#whats-a-reasoning-model","title":"What's a Reasoning Model?","text":"<p>I guess I want to start this conversation by defining what a thinking/reasoning model is. The easiest way to define it is that it is a model specially trained and designed for thinking through problems. </p> <p>Now, the more elaborate way of going about this definition is to look into the information we have about what differs between these models and the traditional models we are used to like ChatGPT or Claude.</p> How Reasoning models differ from Traditional LLMs <p>Reasoning models differ from traditional LLMs in several key ways:</p> <ul> <li> <p>Computational Approach: While traditional LLMs generate text in a fixed number of passes and are optimized for efficiency, reasoning models allocate variable computation time based on problem complexity and implement explicit \"thinking\" phases.</p> </li> <li> <p>Problem-Solving Methods: Traditional LLMs rely primarily on pattern recognition, while reasoning models utilize explicit step-by-step reasoning processes and show their work through intermediate steps.</p> </li> <li> <p>Training Methodology: Reasoning models are trained on specialized datasets focused on reasoning tasks and use reinforcement learning with rewards specifically for reasoning quality.</p> </li> <li> <p>Output Characteristics: Reasoning models produce more structured, methodical responses with explicit verification steps and self-critique, prioritizing logical correctness over natural-sounding language.</p> </li> <li> <p>Chain of Thought (CoT) Processing: Models like o1 and DeepSeek R1 implement extensive chain-of-thought reasoning, breaking down complex problems into manageable steps before providing answers.</p> </li> </ul> <p>The way I like to think about them for the purpose of building stuff with them is this: </p> <p>It's a model that gives you almost guaranteed better performance at the expense of latency and cost.</p> <p>So it's essentially a better yet slower model. </p> <p>I think looking at it from this perspective will help us define a framework for thinking about them effectively.</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#how-do-we-know-theyre-better","title":"How do we Know they're Better?","text":"<p>Well, if you go to  Artificial Analysis  and check out their independent evaluation of different models, you will see that their performance against other models for different indexes of capabilities like intelligence (performance in intelligence related tasks) almost always come out on top.</p> <p>According to recent benchmarks, reasoning models consistently outperform traditional LLMs on tasks requiring complex logical thinking, mathematical problem-solving, and coding challenges. For example, DeepSeek R1 has demonstrated superior performance on mathematical reasoning compared to many general-purpose models, while Claude 3.7 Sonnet with extended thinking enabled shows remarkable improvements over its standard mode.</p> <p>The performance gap is particularly noticeable in: - Complex mathematical problems - Multi-step logical puzzles - Detailed code generation with explanations - Step-by-step problem-solving in scientific domains</p> <p>However, it's worth noting that this superior performance comes with trade-offs in speed and computational cost, which I'll discuss later.</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#which-models-we-can-officially-call-reasoning-models","title":"Which Models we can officially call Reasoning Models?","text":"<p>Let's make a little list of the current leading reasoning models:</p> <ol> <li>Deep Seek R1 - An open-source 671B parameter model that excels in mathematical reasoning and has distilled versions available</li> <li>OpenAI o-series models - Including o1, o1-mini, o1-pro, o3-mini, and o3-mini-high, specifically designed for extended reasoning</li> <li>Claude 3.7 Sonnet - With \"extended thinking\" mode activated, becoming a hybrid reasoning/general model</li> <li>Grok-3 - xAI's latest model with enhanced reasoning capabilities</li> <li>Gemini 2.0 Flash/Advanced - Google's reasoning-focused models</li> <li>Sky T-1 - A relatively new Chinese reasoning model from SkyWork</li> <li>Baidu's ERNIE 4.0-Reasoning - Launched in March 2025 as a competitor to DeepSeek R1</li> <li>Tencent's T1 - A reasoning model introduced in early 2025</li> <li>Llama-3-70B-Instruct - While not explicitly marketed as a reasoning model, it shows strong reasoning capabilities</li> </ol> <p>This list is continually growing as more companies release specialized reasoning models to compete in this rapidly evolving space.</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#when-should-you-use-reasoning-models","title":"When Should You Use Reasoning Models?","text":"<p>Reasoning models shine in specific scenarios, but they're not always the right choice. Here's my take on when they make sense:</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#1-complex-problem-solving","title":"1. Complex Problem-Solving","text":"<p>If your use case involves solving intricate problems that require breaking down complex logic into steps, reasoning models are your best bet. They excel at:</p> <ul> <li>Mathematical challenges</li> <li>Logic puzzles</li> <li>Scientific reasoning</li> <li>Step-by-step explanations</li> </ul>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#2-code-generation-and-analysis","title":"2. Code Generation and Analysis","text":"<p>For software development tasks requiring thoughtful analysis, reasoning models offer significant advantages:</p> <ul> <li>Writing complex algorithms</li> <li>Debugging with systematic approaches</li> <li>Explaining code behavior comprehensively</li> <li>Architecture design with logical justification</li> </ul>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#3-high-stakes-decision-support","title":"3. High-Stakes Decision Support","text":"<p>When accuracy and reliability are paramount:</p> <ul> <li>Financial analysis where errors could be costly</li> <li>Medical reasoning requiring careful consideration</li> <li>Legal analysis with logical chains of thought</li> <li>Risk assessment requiring thorough evaluation</li> </ul>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#4-educational-applications","title":"4. Educational Applications","text":"<p>For tools designed to teach or explain complex topics:</p> <ul> <li>Tutorial systems requiring step-by-step instruction</li> <li>Math problem solving with shown work</li> <li>Scientific concept explanations</li> <li>Knowledge exploration with logical connections</li> </ul>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#when-traditional-llms-might-be-better","title":"When Traditional LLMs Might Be Better","text":"<p>Despite their impressive capabilities, reasoning models aren't always the optimal choice:</p> <ol> <li>Speed-Critical Applications: When response time matters more than deep thinking</li> <li>Creative Content Generation: For stories, marketing copy, or creative writing</li> <li>Casual Conversation: When natural dialogue flow is more important than rigorous logic</li> <li>High-Volume, Simple Tasks: For straightforward, repetitive tasks where the computational overhead isn't justified</li> </ol>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#the-performance-latency-trade-off","title":"The Performance-Latency Trade-off","text":"<p>This is perhaps the most critical consideration when deciding whether to implement reasoning models. They typically:</p> <ul> <li>Take significantly longer to respond (sometimes 5-10x longer)</li> <li>Consume more tokens (and therefore cost more)</li> <li>Require more computational resources</li> <li>Provide more thorough, accurate results for complex problems</li> </ul> <p>In my experience, this trade-off means you should be selective about when to deploy reasoning capabilities. For many applications, a hybrid approach works best:</p> <ul> <li>Use standard LLMs for straightforward queries and initial interactions</li> <li>Switch to reasoning modes for complex questions</li> <li>Allow users to choose whether they want quick answers or deeper analysis</li> </ul>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#how-to-prompt-reasoning-models-effectively","title":"How to Prompt Reasoning Models Effectively","text":"<p>Interestingly, prompting reasoning models often requires a different approach than traditional LLMs. Based on OpenAI's official guidance and my own experimentation:</p> <ol> <li> <p>Keep prompts simple and direct - Contrary to what you might expect, you often don't need to explicitly tell reasoning models to \"think step by step.\" They're already designed to do this.</p> </li> <li> <p>Structure complex problems clearly - Use delimiters and clear formatting to separate different parts of your input.</p> </li> <li> <p>For mathematical or logical problems, provide:</p> </li> <li>Clear problem statements</li> <li>Relevant context</li> <li> <p>Expected output format</p> </li> <li> <p>Model-specific considerations:</p> </li> <li>For OpenAI's o-series, \"developer messages\" replace traditional system messages</li> <li>Claude 3.7 Sonnet requires explicit activation of extended thinking mode</li> <li> <p>DeepSeek R1 excels with mathematical reasoning when given clean, well-structured problems</p> </li> <li> <p>Use verification steps for critical applications - Ask the model to verify its own work when accuracy is paramount.</p> </li> </ol>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#implementation-considerations","title":"Implementation Considerations","text":"<p>If you're planning to implement reasoning models in your applications, consider these practical factors:</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#1-cost-management","title":"1. Cost Management","text":"<p>Reasoning models typically: - Process more tokens per request - Take longer to generate responses - May require premium API access</p> <p>Implement strategies like: - Selective use for complex queries only - Caching common reasoning patterns - User-controlled access to reasoning capabilities</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#2-latency-handling","title":"2. Latency Handling","text":"<p>The extended processing time requires thoughtful UX design: - Implement streaming for progressive response display - Show intermediate reasoning steps as they're generated - Provide clear user expectations about response times</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#3-hybrid-architectures","title":"3. Hybrid Architectures","text":"<p>Consider architectures that combine: - Traditional LLMs for routine queries - Reasoning models for complex problems - Distilled reasoning models as a middle ground - Human review for critical applications</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#4-the-role-of-distilled-models","title":"4. The Role of Distilled Models","text":"<p>An exciting development is the emergence of distilled reasoning models like DeepSeek-R1-Distill-Qwen-32B, which: - Retain 80-95% of the reasoning capabilities of larger models - Run on more modest hardware - Offer faster inference times - Provide a cost-effective middle ground</p> <p>For many practical applications, these distilled models hit the sweet spot between performance and efficiency.</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#the-future-of-reasoning-models","title":"The Future of Reasoning Models","text":"<p>We're just at the beginning of the reasoning model era, and I expect rapid developments:</p> <ul> <li>More efficient reasoning approaches that reduce latency</li> <li>Specialized reasoning models for specific domains (medical, legal, financial)</li> <li>Improved distillation techniques preserving more reasoning capabilities</li> <li>Better integration of reasoning with multimodal capabilities</li> </ul> <p>The competition between open-source models like DeepSeek R1 and proprietary offerings like OpenAI's o-series is driving innovation at a remarkable pace, with the performance gap between them narrowing significantly.</p>"},{"location":"blog/2025/03/23/reasoning-models-when-to-use-them-what-are-they-good-for/#conclusion","title":"Conclusion","text":"<p>Reasoning models represent a significant advancement in AI capabilities, offering new possibilities for applications requiring thoughtful, methodical problem-solving. They're not a replacement for traditional LLMs but rather a specialized tool for specific use cases where depth of thinking matters more than speed.</p> <p>When implementing them, focus on: 1. Selecting the right use cases where their advantages justify the trade-offs 2. Designing user experiences that manage expectations around response times 3. Considering hybrid approaches that leverage different model types appropriately 4. Exploring distilled models as a balanced option for many applications</p> <p>As these models continue to evolve, they'll enable increasingly sophisticated applications that tackle problems previously beyond the reach of AI systems. The key is understanding their unique characteristics and implementing them thoughtfully where they add the most value.</p> <p>What are your experiences with reasoning models? Have you found particular applications where they shine? Let me know in the comments below!</p>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/","title":"Agents, Planning, Evaluation and AI Index Reports 2025","text":""},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#agents-planning-evaluation-and-ai-index-reports-2025","title":"Agents, Planning, Evaluation and AI Index Reports 2025","text":"<p>I recently compiled a reading list of a few articles, reports and interesting content to read as a single .pdf.</p> <p>The highlights of this read were:</p> <ol> <li>Agents by Chip Huyen</li> <li>A State of AI Report by McKinsey</li> <li>The Anthropic Economic Index for Claude 3.7 Sonnet</li> <li>Anthropic Education Report: How University Students Use Claude</li> <li>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</li> </ol> <p>If you want the full list of reads check out the compilation pdf I put up here:</p> <p>Download the full compilation here</p> <p>Now, I want to write about the main insights from this read.</p>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#planning-failures-evaluation-and-tool-selection-tips","title":"Planning Failures, Evaluation and Tool Selection Tips","text":"<p>In Huyen's amazing piece she oulines a few things that for me are not obvious when it comes to thinking about agents, those are modes of planning failures in agents and tips for selecting tools.</p>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#planning-failures","title":"Planning Failures","text":"<p>Agents might fail in the following way:</p> <ol> <li>Invalid tool - where the agent generates a plan with a tool that is not available.</li> <li>Valid tool but invalid parameters - the agent generates a plan with an appropriate tool but it gives parameters to it in the wrong way (like the tool might accept 2 parameters and the agent gives it one).</li> <li>Valid tool but invalid parameter values - Same as before but in this case the agent gives the right amount and type of parameters but these are just the incorrect values.</li> <li>Goal failure - the agent might miss the task all together or not abive by the requirements properly. </li> </ol> <p>Interesting points about goal failure: a. Taking too long - that sometimes the agent might solve a task, but it might take so long that this latency in itself is a failure (like missing a deadline). b. Reflection failure - when the agent thinks it solved the task when it hasn't</p>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#evaluation-for-planning-failures","title":"Evaluation for Planning Failures","text":"<p>To evaluate for planning failures one could generate a planning dataset where each example is a pair of task and tool inventory, where for each task the agent is used to generate a certain number of plans and then compute some interesting metrics:</p> <ol> <li>Out of all generated plans, how many are valid?</li> <li>For a given task, how many plans does the agent have to generate to get a valid plan?</li> <li>Out of all tool calls, how many are valid?</li> <li>How often are invalid tools called?</li> <li>How often are valid tools called with invalid parameters?</li> <li>How often are valid tools called with incorrect parameter values?</li> </ol> <p>All of those attempt to find the quantifiable elements of this subjective problem of measuring performance for the planning capabilities of an agent. </p> <p>The idea here being that, when you analyze the agen'ts outputs for patterns you should look at things like: \"What types of tasks does the agent fail more on?\", any hypothesis as to why?. What tools does it fail more often?. </p> <p>One can improve an agent's ability to use hard tools by improving upon:</p> <ol> <li>Prompting</li> <li>Giving it more examples</li> <li>Fine-tuning</li> </ol>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#tool-selection-tips","title":"Tool Selection Tips","text":"<p>Chip Huyen's article gives these 4 cool tips for tool selection:</p> <p></p>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#insights-from-evalgen-paper","title":"Insights from EvalGen Paper","text":"<p>Luckly enough, despite my reading list being a somewhat random compilation of materials, I accidently put together 2 quite relevant reads back to back, Chip Huyen's Agent article and this really cool 2024 paper: \"Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences\". </p> <p>This paper is interesting because it takes on the challenge of looking at the subjective nature of \"criteria\" when addresssing LLM evaluation. They talk about this concept of \"criteria drift\" where when users face llm outputs that changes and refines the criteria for evaluating them altogether. </p> <p>To me it's interesting this idea that reviewing LLM outputs changes what users think the evaluation criteria should be. I guess it makes sense to think about criteria for what is a good output to be something dynamic and iterative rather than a rigid/written in stone definition.</p> <p>From my ChatGPT conversations about this paper we came up with these insights from the paper:</p> EvalGen Paper Insights <ol> <li> <p>\ud83d\udd01 Evaluation Criteria Evolution</p> <p>Core Finding: Evaluation criteria are dynamic, not static. They evolve as reviewers encounter more LLM outputs.</p> <p>Implication: Build evaluation systems that support iterative refinement. Initial rubrics should be treated as starting points, not final standards. Expect early review cycles to reshape your understanding of quality.</p> </li> <li> <p>\ud83e\udde0 Human Standards Development</p> <p>Core Finding: Humans develop evaluation norms through the review process itself.</p> <p>Implication: Design validation workflows (e.g., reflection agents, human-in-the-loop scoring) with calibration periods. Enable reviewers to co-develop standards rather than enforcing rigid rubrics from the start.</p> </li> <li> <p>\ud83d\udd0d Output-Influenced Judgment</p> <p>Core Finding: LLM outputs directly influence how validators assess quality.</p> <p>Implication: Be mindful of feedback loops where validator criteria adapt to model behavior rather than external standards. Monitor for potential reinforcement of model quirks.</p> </li> <li> <p>\ud83e\udded Transparency Over Consensus</p> <p>Core Finding: Task interpretation evolves through discussion, not just through right/wrong determinations.</p> <p>Implication: In multi-agent systems, prioritize surfacing reasoning and disagreements over achieving consensus. Build mechanisms to track why decisions are made.</p> </li> <li> <p>\ud83d\udcca Tracking Subjectivity</p> <p>Core Finding: Evaluation inherently involves subjective judgment without absolute ground truth.</p> <p>Implication: Implement meta-evaluation tools (e.g., rationale recording, agent reflection prompts) to make subjectivity traceable and transparent.</p> </li> </ol>"},{"location":"blog/2025/04/10/agents-planning-evaluation-and-ai-index-reports-2025/#insights-from-anthropics-claude-37-sonnet-economic-index","title":"Insights from Anthropics Claude 3.7 Sonnet Economic Index","text":"<p>Anthropic has been putting out some awesome reports on AI usage. A recent one that was on my reading list for way too long was this:</p> <p>Anthropic Economic Index: Insights from Claude Sonnet 3.7</p> <p>In it they outline how different occupations use Claude 3.7 Sonnet and its new extended thinking mode.</p> <p>The highlights of this article for me were:</p> <p>Increase in usage share for math/coding and education but a decrease for arts and media! </p> <p>I think this decrease might relate to copywright infringement laws and overall distrust with big tech companies.</p> <p> Image taken from: Anthropic Economic Index: Insights from Claude Sonnet 3.7 </p> <p>Increase in learning usage (a type categorized as augmentation) but also an increase in directive usage (when you ask for what you want directly) in the automation group + a decrease in task iteration</p> <p> Image taken from: Anthropic Economic Index: Insights from Claude Sonnet 3.7 </p> <p>In my head this could be explained as:</p> <pre><code>a. Increase in learning usage because people are learning about the powers of models now and starting to use it (I know so many people that don't use the top models and think we are still in 2022...)\n\nb. Increase in directive usage might be a mix of people getting lazier and trusting more the powerful models coming out\n\nc. Decrease in task iteration I associate with the overall improvement in the quality of the models as a whole\n</code></pre> <p>IT people use it the most for feedback loops (no surprises here...)</p> <p>Copy writers and editors use it the most for task iteration - I thought software engineers were going to win this one...</p> <p>Most occupations are using AI in only a small share of their tasks.</p> <p> Image taken from: Anthropic Economic Index: Insights from Claude Sonnet 3.7 </p> <p>I had to double check with ChatGPT this one, but in this graph they coined \"depth of task usage\" they plot the minimum fraction of tasks in use on the x-axis and the % of occupations on the y-axis, showing that AI usage is still reserved for a small share of tasks across most occupations, which I think is a mix of being early still and the adaptation to this new worklow takes some adjustment.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/","title":"AI Tools for Learning, Studying and Research","text":""},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#goal","title":"Goal","text":"<p>In this piece I go over thoughts and techniques for applying AI tools to learning, studying and academic research workflows.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#fuzzy-principles-for-applying-ai-tools-to-learn-stuff","title":"Fuzzy Principles for Applying AI Tools to Learn Stuff","text":"<p>My view on the topic of using AI to learn and study is:</p> <ul> <li> <p>You can only learn effectively by applying focused and intentional effort into something</p> </li> <li> <p>The amount you trully learn is correlated with how much intentional effort you put into it, that HAS NOT CHANGED WITH AI</p> </li> <li> <p>What has changed is that now we have tools to boost the efficiency of the time we spend on something intentionally</p> </li> <li> <p>The problem, when it comes to AI usage, is that it's use invites bad habits and incentivizes too much cognitive offloading, which can inherently lead to less learning.</p> </li> <li> <p>So if you use AI in such a way where you become less intentional and put in less mental effort, you learn less, but if you use it right, and you can self-regulate the impulse to offload too much to AI, you can learn a whole lot.</p> </li> <li> <p>Intelligent offloading to AI is a critical meta skill in the new age</p> </li> </ul> <p> Image generated with GPT-4o</p> Example Bad and Good Prompts Bad Prompt Good Prompt Write me an essay about:{X} I'm working on this essay about {X} I got stuck on this part regarding {Y} because I can't seem to find a perspective/angle on it, can you suggest 3 ways to get me unstuck? I want to write an essay about {X} but I don't have any background knowledge on the topic, which questions should I be asking to get started? Can you take a look at this essay I wrote: {ESSAY INPUT} and give me feedback on it based on these materials I'm working through: {MATERIALS}? Write the feedback as instructive and actionable bullet points. I have this coding problem: {PROBLEM INPUT} write the code to fix it: I'm trying to figure out this coding problem: {PROBLEM INPUT}, can you identify what am I doing wrong and explain it without code? I got stuck in this coding problem: {PROBLEM INPUT} can you break down the solution into questions and ask them 1 by one so I can figure out the solution by myself? Write each as an individual bullet point. Summarize this article in simple terms: I read this article: {FULL ARTICLE} and this is what I understood: {YOUR OWN SUMMARY OF THE ARTICLE}, can you identify things I missed? I'm studying this: {YOUR STUDY TOPIC}. Extract the main insights from this article: {FULL ARTICLE} that relate to my research and provide quotes that validate each insight. <p>The examples outlined below showcase this idea of striving for learning through active engagement and critical dialogue where rather than accept blindly whatever the AI gives you, you stop to reflect, ask for follow ups, question it , and so on.</p> <p>The idea is that by understanding the dangers, we can strive to mitigate its negative effects and learn a healthy path towards trully better learning with AI.</p> <p>An amazing piece on this topic if you're interested is in this piece by \"The Guardian\".</p> <p>I also like this piece from the \"The Stanford Daily\" by Divya Ganesan discussing how one can avoid the \"AI temptation zones\" and use AI as a tool for critical thinking.</p> <p>Funny thing is that after listening to this article while walking on the park I did a little AI session on it which I think exemplifies perfectly the type of AI usage I want to advocate for, you can check that out here.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#some-fake-math-to-back-it-up","title":"Some Fake Math To Back It Up","text":"<p>If before you spent an hour to learn X amount of information and knowledge, now you can spend that same hour with same intentional effort to learn p * X where p is a multiplier associated with proper usage of technological tools. </p> <p>However there is a catch, if poorly used, AI tools can undermine learning by creating these dangerous incentives towards things like guess and check approaches or copy pasting essays and so on.</p> <p>To put it in some fake math terms:</p> Learning Output = p \u00d7 X <p> X is the baseline amount of information/knowledge learned in one hour     </p> <p> p = f(proper tool usage) is the efficiency multiplier from using AI tools effectively     </p> <p> p &gt; 1 when AI tools are used properly     </p> <p> p = 1 represents traditional learning without AI tools     </p> <p> p \u2192 ? represents the degradation of learning when AI tools are misused, creating perverse incentives that can undermine knowledge acquisition     </p> <p></p> <p> Image generated by me! :)</p> <p>Note on this graph is that I don't mean learning is linear, I'm just arguing for the potential of efficient usage of AI.</p> <p>Think about the fact that if you have time X to do task Y you can either waste it on watching say a full YouTube video from beginning to end, or you can take the video transcript, feed into AI and then extract exactly what you need with timestamp references to the relevant parts, so that you can directly tackle your problem rather than waste time with information you don't need. Why would you not do it? There is a qualitative difference between that and asking AI to do your homework.</p> <p>In my view, the goal is to: </p> <p>Boost efficiency of the intentional effort we put in to learning anything by systematically addressing the pragmatic challenges and obstacles of each of proper strategic use of AI for learning, studying and research</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#practical-definitions-tooling-landscape","title":"Practical Definitions &amp; Tooling Landscape","text":"AI &amp; LLM Terminology Core Concepts <ul> <li>AI Tool: Software that exposes one or more AI models via UI or API to perform tasks (e.g., text generation, summarization, image creation).</li> <li>Model: A trained neural network that maps inputs to outputs (e.g., text \u2192 text, image \u2192 caption).</li> <li>LLM (Large Language Model): A transformer\u2011based model (often billions of parameters) that predicts and generates text token by token.</li> <li>Transformer: Neural architecture using self\u2011attention to capture dependencies across a sequence; backbone of modern LLMs.</li> <li>Token: Discrete unit of text (word piece, character) that the model processes; vocabulary element.</li> <li>Context Window: Maximum number of tokens the model can \"see\" in one call; sets the input+output limit.</li> <li>Prompt: User\u2011provided text (and instructions) that conditions the model's output.</li> <li>Prompt Engineering: Crafting prompts (templates, examples) to steer model behavior and improve output quality.</li> </ul> Advanced Concepts <ul> <li>Zero\u2011/Few\u2011Shot: Prompting without (zero\u2011) or with a small set of examples (few\u2011shot) to teach the model a task.</li> <li>Embedding: Fixed\u2011length vector representing text (or images) in continuous space, used for similarity and retrieval. ?</li> <li>Retrieval\u2011Augmented Generation (RAG): Technique that retrieves relevant documents via embeddings and feeds them into the LLM to ground its answers. ?</li> <li>Agent: Script or workflow that chains multiple prompts, API calls, or tools (search, calculator) to perform complex tasks. ?</li> <li>Structured Output: Constraining model responses into predefined formats (JSON, tables) for reliable parsing and downstream use.</li> <li>Hallucination: Model's confident but incorrect or fabricated information; mitigated by grounding, citations, or checks. ?</li> <li>Inference: The act of running the model on new inputs to produce outputs. ?</li> <li>API Call: Programmatic request to an AI service, submitting prompt and receiving model output. ?</li> </ul> AI Tooling Landscape <ul> <li>ChatGPT<ul> <li>Custom GPT (or Custom Assistant): User\u2011configured agent built via templates or fine\u2011tuning to automate repeatable workflows. ?</li> <li>Projects</li> <li>Canvas</li> </ul> </li> <li>Claude<ul> <li>Artifacts</li> <li>Projects</li> </ul> </li> <li>Gemini</li> <li>Llama</li> <li>Perplexity</li> <li>Elicit</li> <li>DeepSeek</li> <li>Ollama</li> <li>NotebookLM</li> </ul>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#foundational-tool-skills","title":"Foundational Tool Skills","text":"<p>Hopefully at this point you're convinced there is a good way to use AI, so the question is now how? To address this let's start by the tools we need and the skills we aim to develop:</p> <ul> <li>Prompting (ChatGPT, Claude, Gemini, Llama, Deep Seek)</li> <li>CRE Framework for ChatGPT prompting</li> <li>Practical Approach to Prompting</li> <li>Prompt Templating</li> <li> <p>Prompt Improvers like anthropic console's prompt improver</p> </li> <li> <p>Image Generation</p> </li> <li> <p>Deep Research</p> </li> <li> <p>Prompt management tools</p> </li> </ul> <p>After finishing come back and see which were the tools used in fact</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#foundational-meta-skills","title":"Foundational 'Meta-Skills'","text":"<ul> <li>Meta-usage </li> <li>Clarifying</li> <li>Learning to ask for questions</li> <li>Planning with reasoning LLMs</li> </ul> <p>After finishing come back and see which were the meta stuff addressed</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#learning","title":"Learning","text":"<p>To serve a practical end we'll discuss learning as this navigation through the layers of the Bloom Taxonomy.</p> Bloom Taxonomy <p>Bloom's Taxonomy was first articulated in the mid\u201120<sup>th</sup> century as a systematic classification of educational objectives, laying the foundation for decades of instructional design and assessment practices. The original concept emerged from a committee of educators chaired by Benjamin S. Bloom and was codified in a handbook focused on the cognitive domain. This seminal work not only defined six hierarchical levels of cognitive processes but also established parallel frameworks for affective and psychomotor domains, profoundly shaping how learning outcomes are written and evaluated.</p> <p>Sources:      - <code>Bloom, B. S., Engelhart, M. D., Furst, E. J., Hill, W. H., &amp; Krathwohl, D. R. (1956). Taxonomy of Educational Objectives: The Classification of Educational Goals \u2013 Handbook I: Cognitive Domain. London: Longmans.</code></p> <p> Image Generated by the author inspired by Bloom 1956</p> <p>The Bloom Taxonomy is a systematic classification of educational objectives that outlines 6 hierarchical levels of cognitive processes:</p> <ul> <li>Remembering</li> <li>Understanding</li> <li>Applying</li> <li>Analysing</li> <li>Evaluating</li> <li>Creating</li> </ul> <p>Essentially, I want to discuss how we can effectively leverage AI to improve, accelerate and boost the efficiency of each of these learning stages. I also want to emphasize that the bridge I am making here is not scientific, its merely practical, giving us a useful theoretical foundation with which we can work.</p> <p>For each of these I want to provide some workable definition of what we're trying to achieve, and then go on to techniques, tips and effective strategies for using AI to navigate each of these stages.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#remembering","title":"Remembering","text":"<p>For this stage what we want is to have a system to ensure we remember whatever we want. </p> <p>My favorite article on this topic is from the legendary Michael Nielsen: 'Augmenting Long Term Memory' where he outlines his preference and interest for anki flashcards.</p> Summary of Augmented Long Term Memory <ul> <li>Memory is leverage \u2013 Spaced\u2011repetition cuts the lifetime review cost of a fact from ~2\u00a0h (paper flashcards) to 4\u20117\u00a0min, making recall a choice instead of luck.  </li> <li>Anki\u2019s core mechanic \u2013 After every correct answer, the interval before the next review grows (\u2248\u00d72.4). A failure resets the card, enforcing \u201cdesirable difficulty\u201d.</li> <li>Create cards while creating things \u2013 Extract questions from papers, code, conversations or experiments you\u2019re actively working on; skip \u201cjust\u2011in\u2011case\u201d trivia to avoid orphan cards.</li> <li>Atomic questions &gt; fuzzy prompts \u2013 One idea per card, no yes/no framing; split big facts into multiple Cloze/Q\u2011A cards for sharper recall.</li> <li>One deck, mixed domains \u2013 Tags are fine, but reviewing everything together keeps sessions short (10\u201120\u00a0min/day) and sparks cross\u2011domain associations.</li> <li>Retrieval first, re\u2011reading last \u2013 Regular testing at the edge of forgetting strengthens memory far more than rereading notes; convert frequent ChatGPT errors into new cards.</li> <li>Declarative \u2194 procedural bridge \u2013 SRS primes commands, formulas and language; cement them by performing a real micro\u2011task right after review.</li> <li>Cognitive backdrop \u2013 Builds on the Ebbinghaus forgetting curve and distributed\u2011practice research: spaced tests flatten the decay curve and enlarge working\u2011memory \u201cchunks\u201d, accelerating higher\u2011order thinking.</li> <li>Keep the setup simple \u2013 Vanilla Anki (Q\u2011A + Cloze) delivers 95\u00a0% of the value; chase plugins and automation after the habit sticks.</li> </ul> <p>Source: Augmenting Long-term Memory</p> <p>To follow along you should have some flashcard software that you can use, I recommend Anki Flashcards, but similar approaches can be achieved with any other.</p> <p>I love anki flashcards so here we go. Let's talk about some techniques I use on a weekly basis to help memorize all sorts of information.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#1-from-chatgpt-chat-to-anki","title":"1. From ChatGPT Chat to Anki","text":"<p>Prompt:</p> <pre><code>'Create {{PUT AMOUNT OF CARDS}} anki flashcards for the content below in the format:\n\n    ''''''&lt;front content&gt;;&lt;back content&gt;''''''.\n\n    Try to capture all the main information and craft cards that can encode that effectively.\n\n\n{{PASTE CONTENT TO ANKIFY}}\n</code></pre> <p>Expected Output</p> <pre><code># For any conversation when there are more than just a few facts I want to memorize I use this prompt \n# to just generate a simple .txt file with the structure you see below\nback question1 ;front answer 1\nback question2 ;front answer 2\n....\n</code></pre> <p>Which allows me to upload them directly to anki in bulk instead of having to create individual cards, imagine having to study for a biology exam and having to write all the cards yourself.</p> <p>My pro tip is to not go crazy trying to ankify everything because that can lead to a lot of inneficient studying down the line, but to cultivate a skill of learnign what you should memorize, and then using AI to facilitate the process when you have to memorize a lot of stuff from the same source.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#2-quiz-from","title":"2. Quiz From","text":"<p>Beyond just creating flashcards, you need to actually practice retrieval, given that at this stage is pretty much stablished that retrieval works quite well if done right. </p> Effectiveness of Retrieval Practice on Knowledge Retention <p>Retrieval practice, or self-testing, has been shown to be an effective strategy for enhancing learning and retention across various academic disciplines (Richmond et al., 2022; Maddox &amp; Balota, 2015). Both overt and covert retrieval practice can benefit retention equally (Smith et al., 2013). The effectiveness of retrieval practice may be influenced by factors such as spacing (Maddox &amp; Balota, 2015), timing (Matayoshi et al., 2020), and the difficulty of retrieval (Greving et al., 2020). However, some studies have found mixed results, with one study in a MOOC environment showing no significant effect of retrieval practice (Davis et al., 2018). Importantly, when given control over their learning, students often fail to utilize retrieval practice effectively, leading to poor retention (Karpicke, 2009). Retrieval practice can improve both higher- and lower-level thinking (Richmond et al., 2022) and enhance subsequent encoding during study (Karpicke, 2009). Additionally, retrieval practice can benefit the accuracy of self-assessment and overall test performance (Pilotti et al., 2009).</p> <p>Source: Elicit quick research report</p> <p>For quizzes what I like to do is literally ask ChatGPT/Claude/Gemini to quiz me on topic X, usually quizzing yourself directly on the chat can be useful as a kind of screening stage for the stuff you tend to forget, and for which you could potentially  create anki cards for.</p> <p>Prompt:</p> <pre><code>I am studying for {DESCRIBE THE CONTEXT OF YOUR QUIZ} and I want you \nto quiz me on this content:\n{PASTE THE CONTENT YOU WANT TO BE QUIZZED ON LIKE A PDF}\n\nAsk me 3 questions at a time and wait for my reply before providing feedback.\n</code></pre> <p>The pro tip here is to be very customizable regarding how your quizzed and  the nature and format of the questions and the feedback you expect to get.</p> Custom Specialized Prompts for the Remembering Stage <pre><code>1\u00a0\u2014\u00a0Rapid\u2011Fire Recall Quiz\nPrompt template\n\u201cAsk me 10 single\u2011sentence questions that simply check whether I can name, define or list key points about\u00a0&lt;topic&gt;. After each answer I give, tell me immediately if I\u2019m right, supply the correct answer if needed, then move to the next question. End with a scoreboard.\u201d\n\nWhy it works\u00a0\u2013 immediate feedback plus spacing; uses \u201cname/define/list\u201d verbs tied to the remembering tier.\u200b\n[Whatfix](https://whatfix.com/blog/blooms-taxonomy/?utm_source=chatgpt.com)\n[Tips at UARK](https://tips.uark.edu/blooms-taxonomy-verb-chart/?utm_source=chatgpt.com)\n\n2\u00a0\u2014\u00a0Cloze\u2011Deletion Flashcards\nPrompt template\n\u201cCreate 20 cloze\u2011deletion cards (Anki\u2011style \u2018fill\u2011in\u2011the\u2011blank\u2019) for the most testable facts about\u00a0&lt;topic&gt;. Return them as a two\u2011column table: Front (sentence with [\u2026] blank) | Back (missing word).\u201d\n\nCloze cards force lexical recall, not mere recognition, while tables paste straight into flashcard apps.\u200b\n[TeachThought](https://www.teachthought.com/critical-thinking/question-stems/?utm_source=chatgpt.com)\n[Salisbury University](https://www.salisbury.edu/administration/academic-affairs/instructional-design-delivery/articles/blooms-taxonomy-action-verbs.aspx?utm_source=chatgpt.com)\n\n3\u00a0\u2014\u00a0Definition\u2011Matching Grid\nPrompt template\n\u201cGive me a shuffled list of 15 terms and 15 concise definitions for\u00a0&lt;topic&gt; in two separate numbered columns so I can draw lines or use drag\u2011and\u2011drop tools to match them.\u201d\n\nMatching is a classic remember\u2011level activity and the two\u2011column format plugs into digital worksheet generators.\u200b\n[ThoughtCo](https://www.thoughtco.com/blooms-taxonomy-questions-7598?utm_source=chatgpt.com)\n[Utica University](https://www.utica.edu/academic/Assessment/new/Blooms%20Taxonomy%20-%20Best.pdf?utm_source=chatgpt.com)\n\n4\u00a0\u2014\u00a0Memory\u2011Palace Hooks\nPrompt template\n\u201cFor each core fact I must memorize about\u00a0&lt;topic&gt;, produce: 1) the fact in one sentence; 2) a vivid, quirky image I can visualize in a memory\u2011palace room; 3) a one\u2011word cue. Limit to 12 items.\u201d\n\nAdds imagery to raw recall, strengthening encoding while staying inside the \u201cremember\u201d objective.\u200b\nworldofinsights.co\n[Whatfix](https://whatfix.com/blog/blooms-taxonomy/?utm_source=chatgpt.com)\n\n5\u00a0\u2014\u00a01\u2011Minute \u201cDump\u201d Drill Generator\nPrompt template\n\u201cGive me a prompt that tells me to write down (from memory, no peeking) everything I know about\u00a0&lt;topic&gt; for exactly 60\u00a0seconds, then supply the authoritative list so I can compare gaps.\u201d\n\nThe timed free\u2011recall (\u201cbrain dump\u201d) pushes retrieval fluency; comparing to the model answer highlights omissions.\u200b\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#3-quizzing-with-quotes","title":"3. Quizzing with Quotes","text":"<p>When quizzing yourself with a PDF for example the #1 tip is for any statement given ask for a 'quote that validates it', for example</p> Prompt for Quizzing with Quotes <p><pre><code>Prompt:\nYou are an expert instructional\u2011designer.\n\nI will paste text from a PDF (any length, possibly in chunks).  \nCreate **higher\u2011order multiple\u2011choice questions** that check genuine comprehension (analysis, causation, implications, synthesis).\n\n**Format your reply exactly like this (no JSON, no extra commentary):**\n\n1. **[Question]**  \n  A) \u2026  \n  B) \u2026  \n  C) \u2026  \n  D) \u2026  \n\nTHE OUTPUTS LIKE BELOW SHOULD BE PROVIDED AFTER GETTING THE USER's answer FIRST:\n  **Answer:** C  \n  **Supporting quote (page\u00a0#, paragraph\u00a0#):** \u201cExact \u2264\u00a040\u2011word excerpt that proves the correct answer.\u201d\n\n2. **[Question]**  \n  \u2026\n\n\u2026and so on until the user says STOP, or asks for modifications on the quiz.\n\n**Rules**\n\n- Choices must be plausible, mutually exclusive, and cover common misconceptions.  \n- Do **not** repeat wording from the stem in the correct choice.  \n- Draw questions from across the whole document so every main section is tested.  \n- Quotes: copy verbatim\u2014including punctuation\u2014from the PDF; shorten with ellipses only if necessary for brevity.  \n- Do **not** provide quotes for wrong choices or add explanations besides the single supporting quote.  \n- Keep the presentation clean\u2014just the numbered list above, nothing else.\n- Ask ONE QUESTION at a time, wait for answer, then give rich FEEDBACK and move to the next question\n</code></pre>   Example output:    Screenshot take from https://chatgpt.com/</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#4-summarize-with-references-to-sources","title":"4. Summarize with References to Sources","text":"<p>You can also ask for summaries that refer to the original source for quick reference and validation:</p> <pre><code>Prompt: \nSummarize the following content in a format that presents a bullet point followed by a related key quote to illustrate each summarized bullet point:\n\n{{ SOURCE CONTENT LIKE A PDF }}\n</code></pre> <p>You can do the same even with Youtube videos:</p> <pre><code>Prompt:\nSummarize this Youtube video the transcript is below:\n{INPUT YOUTUBE TRANSCRIPT}\nNow the summary should be about the main topics discussed\nregarding algebraic topology and the structure of the summary\nshould follow:\n\n'''\n1. {Core idea explanation}\n- youtube link refering to the timestamp that validates the idea\n\n2. {Core idea explanation}\n- youtube link refering to the timestamp that validates the idea\netc...\n'''\n\nto create the youtube links with the right timestamps consider this is the link for the original video:\n{INPUT YOUTUBE LINK}\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#5-ai-powered-search-for-validation-of-summarized-information-access","title":"5. AI-Powered Search for Validation of Summarized Information Access","text":"<p>When you're quizzing yourself or creating flashcards, obviously you need to make sure that the information holds, to guarantee that you can use AI-powered search to validate sources for questions and information that you think might not be as widely available online.</p> <p>You can leverage search + smart prompting to create flashcards that accompany sources.</p> <p>Search should also serve as validation specially for topics you think might be less available online and therefore the model's training might not be able to properly prepare the model to respond.</p> <pre><code>  Prompt:\n  'Create {NUMBER OF CARDS} anki flashcards for the content below in the format:\n\n      ''''''&lt;front content&gt;;&lt;back content&gt;;source as link if available or reference to doc title or origin''''''.\n\n      Try to capture all the main information and craft cards that can encode that effectively.\n\n\n  content:\n  {INPUT RAW CONTENT}\n\n  Make sure to: \n  {INPUT ANY CUSTOM INSTRUCTIONS}\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#6-leverage-image-generation-for-memorizing-procedures","title":"6. Leverage Image Generation for Memorizing Procedures","text":"<p>Now, beyond just prompting we can leverage other generative capabilities of AI models to complement  memorization of specific types of information or processes. For example, one thing I like to do is to leverage GPT-4o image generation for memorizing procedures.</p> <pre><code>Prompt:\n{INPUT RECIPE/PROCEDURE WITH CONCRETE STEPS}\n\nCan you Create a helpful visual to help me remember this ?\nMake sure to include\n1. All the ingredients\n2. All the instruction steps through relevant imagery \n3. Make it a like in a graphic novel like style\n</code></pre> <p> Image generated with GPT-4o</p> <p>You can check out the full conversation with ChatGPT here.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#tips-for-better-remebering-with-ai","title":"Tips for Better Remebering with AI","text":"<ul> <li>Never generate cards blindly, if you don't have agency over the process you might recall some facts but won't be able to integrate that into anything</li> <li>Don't try to memorize too much at first, get an overview first by chatting with the model about the topic of interest</li> <li>Leverage Programmable Attention</li> </ul> Programmable attention and priming <p>Programmable attention, a concept explored by Andy Matuschak, refers to systems that dynamically guide your focus over time, automating the scheduling of small tasks\u2014like flashcard reviews or creative exercises\u2014to optimize learning and productivity. Tools such as spaced repetition systems (e.g., Anki, Orbit) exemplify this by managing when and what you review, reducing cognitive load and enhancing retention. This approach extends beyond memorization to support creative work and habit formation, integrating with practices like evergreen notes to foster sustained intellectual development.</p> <p>Source: https://notes.andymatuschak.org/Programmable_attention </p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#understanding","title":"Understanding","text":"<ul> <li> <p>Start off with basic brainstorming - example chat</p> </li> <li> <p>Explain it back</p> </li> </ul> <p>Informal (spoken) version   Example from chat:   </p> <p>Formal version</p> <pre><code>Prompt\nI am analyzing a specific article and want to extract and understand its main points. I will share my thoughts with you one at a time. For each thought I share:\n\nGive me immediate and concise feedback.\n\nRefer explicitly to the original article to validate or correct what I\u2019ve said.\n\nKeep your responses focused and factual, pointing out where my interpretation aligns with or diverges from the text.\n</code></pre> <ul> <li>Compare &amp; Contrast <pre><code>Prompt\nI'm studying {X} Prompt me to write a 100\u2011word paragraph comparing and contrasting {CONCEPT A} and {CONCEPT B}, focusing on purpose, process and outcome. After I answer, supply your own comparison with a three\u2011bullet improvement checklist.\n</code></pre></li> </ul> More Prompts for Understanding <p>Differentiate between A and B   <pre><code>Prompt:\nTask 1 (60 s, no notes): List at least three key ways that &lt;concept A&gt; and &lt;concept B&gt; differ in purpose, structure and outcome.\nTask 2: When I\u2019m done, show a concise comparison table (rows = dimensions; cols = A vs B) and highlight any distinctions I missed or got wrong.\n</code></pre>   Quick example   <code>Replace the placeholders with mitosis vs meiosis (biology) or traditional budgeting vs zero-based budgeting (finance).</code>   Identify the main idea of X   <pre><code>Prompt:\nGive me a 120-word, expert-level paragraph on &lt;topic&gt;. Instruct me to rewrite the single, \nclear main idea in \u226420 words without looking back. After my answer, display the\nauthoritative main-idea sentence and emphasise any concepts I omitted.\n</code></pre>   Quick example   <code>Topic could be the Doppler effect (physics) or Maslow\u2019s hierarchy of needs (psychology).</code>   Explain \u201cWhy did \u2026?\u201d   <pre><code>Prompt: \nAsk me: \u2018Why did &lt;event/process&gt; happen, and what were its two most significant consequences?\u2019\nAfter I answer (max 150 words), provide a model explanation organised as Cause \u2192 Mechanism \u2192 Effect 1 &amp; 2, and underline any causal links I missed.\n</code></pre>   Quick example   <code>Fill with Why did the Berlin Wall fall in 1989? or Why does increasing enzyme temperature accelerate then inhibit reactions?</code></p> <p>Paraphrasing short chunks of large materials   <pre><code>Prompt\n\u201cGive me a short authoritative paragraph (\u2248120\u00a0words) about {TOPIC}. Tell me to paraphrase it in my own words without looking back. Then display both texts side\u2011by\u2011side and flag any ideas I dropped or distorted.\u201d\n</code></pre>   Quick Example   <code>\"Give me a short authoritative paragraph (\u2248120 words) about the process of photosynthesis. Tell me to paraphrase it in my own words without looking back. Then display both texts side\u2011by\u2011side and flag any ideas I dropped or distorted.\"</code></p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#applying","title":"Applying","text":"<p>The Applying stage focuses on using knowledge in practical contexts, requiring students to demonstrate understanding  through concrete examples, exercises, and real-world applications.</p> Applying Stage Prompts"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#examples-application","title":"Examples &amp; Application","text":""},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#experimentation","title":"Experimentation","text":"<p><pre><code>Prompt: Give me an idea for an experiment I could do to test\nmy knowledge of {X}.\n</code></pre> <pre><code>Prompt: \"Generate 3 concrete examples of how &lt;concept&gt; applies in real-world scenarios. \nFor each example, explain how it demonstrates the core principles\nof {INPUT YOUR TOPIC OF FOCUS}\n</code></pre></p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#exercises","title":"Exercises","text":"<pre><code>Prompt: \"Create 5 exercises I can use to practice skill {X}.\"\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#study-guide-creation","title":"Study Guide Creation","text":"<pre><code>Prompt: \"Create a comprehensive study guide for {TOPIC} that includes:\n- Key concepts and definitions\n- Practical examples\n- Common misconceptions\n- Practice questions\n- Real-world applications\"\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#contextual-feedback","title":"Contextual Feedback","text":"<pre><code>Prompt: \"Review my understanding of {CONCEPT} and provide feedback that:\n1. Acknowledges my current level of understanding\n2. Points out specific areas for improvement\n3. Suggests practical ways to apply this knowledge\"\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#analysis-of-mechanisms","title":"Analysis of Mechanisms","text":"<pre><code>Prompt: \"Explain why {PROCESS/CONCEPT} works the way it does, breaking down:\n4. The underlying principles\n5. The cause-and-effect relationships\n6. The practical implications\"\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#modification-adaptation","title":"Modification &amp; Adaptation","text":"<pre><code>Prompt: \"How would you modify {PROCESS/SYSTEM} to achieve {SPECIFIC GOAL}? \nConsider:\n7. Current limitations\n8. Required changes\n9. Potential impacts\"\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#instruction-development","title":"Instruction Development","text":"<pre><code>Prompt: \"Create a step-by-step guide for {PROCESS/TASK} that:\n10. Is clear and actionable\n11. Includes important precautions\n12. Provides examples for each step\"\n</code></pre>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#analysing-synthesizing","title":"Analysing &amp; Synthesizing","text":"Prompts for Analysis and Synthesis <p>Summary   <pre><code>Prompt\nYou are an expert content summarizer. You take content in and output a Markdown formatted summary using the format below.\n\nOUTPUT SECTIONS\nCombine all of your understanding of the content into a single, 20-word sentence in a section called ONE SENTENCE SUMMARY:.\n\nOutput the 10 most important points of the content as a list with no more than 16 words per point into a section called MAIN POINTS:.\n\nOutput a list of the 5 best takeaways from the content in a section called TAKEAWAYS:.\n\nOUTPUT INSTRUCTIONS\nCreate the output using the formatting above.\nYou only output human readable Markdown.\nOutput numbered lists, not bullets.\nDo not output warnings or notes\u2014just the requested sections.\nDo not repeat items in the output sections.\nDo not start items with the same opening words.\n</code></pre>   This example was adapted from here</p> <p>Weighing Factors   <pre><code>Prompt\nI read this: {CONTENT} and for this context: {CONTEXT DESCRIPTION}\nthese are the factors I consider to be relevant along with a 1-5 score for their importance {INPUT YOUR OWN FACTOR RANKING},\ngive me feedback on this by contrasting with your own and organizing everything into a table.\n</code></pre></p> <p>Contrasting Perspectives   <pre><code>Prompt\nGiven context {CONTEXT} I think {YOUR PERSPECTIVE} take an opposing view and argue with me helping me flush out arguments and ideas.\nLet's work through small debating sessions focused on one topic at a time.\n</code></pre></p> <p>Re-framing (Contrasting &amp; Reflecting)</p> <p>Prompt <pre><code>I\u2019ll paste a {TEXT_TYPE} on {TOPIC}.  \n\n\u2022 First, **I** will rewrite {NUM_PARAGRAPHS} paragraphs from the perspective of a {LENS_A}; you *only* respond with probing questions that test my framing.  \n\n\u2022 After my revision, **you** produce your own {LENS_A} version **and** a {LENS_B} version, noting three framing shifts between them.  \n\n\u2022 End with one question that forces me to choose which lens better serves my purpose.\n</code></pre></p> <p>Placeholder guide</p> Placeholder What to insert Example <code>{TEXT_TYPE}</code> Kind of source you\u2019ll paste policy brief, lab report, news article <code>{TOPIC}</code> Subject matter urban housing, quantum computing <code>{NUM_PARAGRAPHS}</code> How much you\u2019ll rewrite first two, one <code>{LENS_A}</code> First analytic lens or stakeholder view behavioural economist, UX designer <code>{LENS_B}</code> Second contrasting lens environmental activist, CFO <p>Much fancier example here</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#evaluating","title":"Evaluating","text":"Examples of Prompts for Evaluating Stage <p>Agree\u00a0/\u00a0Disagree</p> <pre><code>Prompt\n\nI\u2019m going to share a {STATEMENT} about {TOPIC}.\n\n\u2022 First, **I** decide whether I *agree* or *disagree* and give a brief rationale (\u2264\u00a0100\u00a0words).  \n\u2022 You reply with  \n  1. one probing question that tests my reasoning and  \n  2. a concise counter\u2011position backed by one piece of {EVIDENCE_TYPE}.  \n\nFinish by asking me to revise or stand firm\u2014and explain why.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{STATEMENT}</code> Claim / assertion Social media worsens teenage mental health. <code>{TOPIC}</code> Subject area adolescent psychology <code>{EVIDENCE_TYPE}</code> Evidence style peer\u2011reviewed study <p>Choosing the Best Option</p> <pre><code>Prompt\nI\u2019ll paste {NUM_OPTIONS} candidate {OPTIONS_TYPE} for solving {PROBLEM}.\n\n\u2022 **I** will rank them best\u00a0\u2192\u00a0worst and add a one\u2011sentence reason for each.  \n\u2022 You then  \n  3. share your own ranking (mark any swaps with \ud83d\udd04),  \n  4. point out one criterion I overlooked and  \n  5. ask how including that criterion might change my decision.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{NUM_OPTIONS}</code> Number 3 <code>{OPTIONS_TYPE}</code> Plans / tools / policies marketing strategies <code>{PROBLEM}</code> Challenge boosting newsletter engagement <p>Determining the Most Effective Approach</p> <pre><code>Prompt\nI\u2019ll provide three approaches ({APPROACH_A}, {APPROACH_B}, {APPROACH_C}) to achieve {GOAL}.\n\n\u2022 Build a comparison grid whose rows = evaluation criteria I list below.  \n\u2022 Leave the grid blank for me to score 1\u20115.  \n\u2022 After I post my scores:  \n  \u2013 add your own,  \n  \u2013 highlight in **red** any criterion where we differ by \u2265\u00a02 points and justify briefly,  \n  \u2013 conclude with an evidence\u2011based verdict on the most effective approach.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{APPROACH_A/B/C}</code> Method names in\u2011house build / outsourced SaaS / open\u2011source stack <code>{GOAL}</code> Desired outcome deploying a data dashboard <p>Opinion\u2011with\u2011Evidence</p> <pre><code>Prompt\nI\u2019ll paste a question: {OPINION_QUESTION}.\n\n\u2022 **I** share my opinion plus at least two supporting reasons or pieces of evidence.  \n\u2022 You evaluate my support by  \n  6. rating each reason for credibility (1\u20115),  \n  7. suggesting one stronger piece of {EVIDENCE_TYPE} and  \n  8. asking a follow\u2011up that deepens the argument.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{OPINION_QUESTION}</code> \u201cWhat is your opinion\u2026?\u201d prompt Should universities scrap final exams? <code>{EVIDENCE_TYPE}</code> Evidence style longitudinal study <p>How Would You Improve This?</p> <pre><code>Prompt\nI\u2019ll paste a {WORK_SAMPLE} on {TOPIC}.\n\n\u2022 First, **I** list three concrete improvements.  \n\u2022 You then  \n  9. score each suggestion for impact (\u2605\u2011scale) and feasibility (\ud83d\udcb0\u2011scale),  \n  10. offer one high\u2011leverage improvement I missed and  \n  11. challenge me to draft an action plan for the highest\u2011impact item.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{WORK_SAMPLE}</code> Draft / design / code snippet executive summary <code>{TOPIC}</code> Subject sustainable packaging <p>Stronger Argument</p> <pre><code>Prompt\nI\u2019ll present two arguments ({ARG_A} vs {ARG_B}) about {ISSUE}.\n\n\u2022 **I** choose which is stronger and justify in \u2264\u00a0120\u00a0words.  \n\u2022 You respond by  \n  12. steel\u2011manning the side I *didn\u2019t* pick,  \n  13. surfacing one hidden assumption in my reasoning and  \n  14. asking whether revising that assumption alters my verdict.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{ARG_A}</code> / <code>{ARG_B}</code> Argument summaries pro vs con on remote work <code>{ISSUE}</code> Debate topic workplace productivity"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#creating","title":"Creating","text":"Example Prompts for Creating <p>Rapid Prototype Planner</p> <pre><code>Prompt\nI\u2019ll describe a {RAW_IDEA} for a {PRODUCT_TYPE}.\n\n\u2022 First, **I** identify the core user need.  \n\u2022 You then  \n  1.  outline a no\u2011code or low\u2011code way to test the idea in \u2264\u00a03 steps,  \n  2.  list one metric for each step to validate success and  \n  3.  suggest one resource (tool, tutorial, template) to accelerate execution.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{RAW_IDEA}</code> Concept / feature description habit\u2011tracking app for ADHD <code>{PRODUCT_TYPE}</code> Category mobile app <p>Story\u00a0Remix</p> <pre><code>Prompt\nI\u2019ll paste raw {FACTS} about {TOPIC}.\n\n\u2022 **I** note the target audience\u2019s reading level.  \n\u2022 You respond by  \n  4.  weaving the facts into a 4\u2011paragraph story arc (setup\u00a0\u2192\u00a0conflict\u00a0\u2192\u00a0climax\u00a0\u2192\u00a0resolution),  \n  5.  embedding an analogy and a vivid sensory detail and  \n  6.  ending with a thought\u2011provoking question to spur reflection.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{FACTS}</code> Bullet list / notes climate change statistics <code>{TOPIC}</code> Subject polar ice melt <p>Teach\u2011Back Mini\u2011Course</p> <pre><code>Prompt\nI want to teach {SUBJECT} to {LEARNERS}.\n\n\u2022 **I** outline one constraint (time, format, prior knowledge).\n\u2022 You then  \n  7.  draft a concise 3\u2011module syllabus,  \n  8.  propose an interactive activity for each module and  \n  9.  provide a quick formative assessment to measure learning gains.\n</code></pre> <p>Placeholder guide</p> Placeholder Insert\u2026 Example <code>{SUBJECT}</code> Topic SQL joins <code>{LEARNERS}</code> Audience description bootcamp graduates"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#studying","title":"Studying","text":"<p>In the start of this article I stated that learning effectively was correlated with the amount of intentional effort you put into it. When discussing the effectiveness of studying I'll frame it as the process of getting the absolute most out of the intentional  effort you put into studying whatever you want to learn. </p> <p>To do that I like to treat studying as the skill of building practice, where practice means a closed loop with a clear feedback where we can easily self-track our progress.</p> <p>To build practice with the help of AI tools I like constructing what I call <code>micro-flows</code>which essentially emcompass:</p> <ol> <li>A simple loop with a learning goal</li> <li>A set of concrete steps</li> <li>Clear Outcomes</li> </ol> <p>Ideally a micro-flow would involve:</p> <ol> <li>A regular environment with predictable regularities</li> <li>A concrete tangible feedback loop</li> <li>A pipeline for gradually increasing challenges</li> </ol> <p>These aim to foster what is known as the goldilocks principle which in cognitive science and developmental psychology refers to this idea where people learn best when the material is neither too easy not too hard but 'just right'.</p> <p>For example for studying pdfs the micro-flow might be:</p> <p> Image generated by me! :)</p> <p>These things I call micro-flows don't have to be rigid and strictly cover those elements, essentially they should be simple loops of learning something with concrete outcomes adapted to the context of the student, for example a simple flow to prepare for a biology exam might look like:</p> <p></p> <p>Image generated by me! :)</p> <p>Building these micro-flows is a skill, and I think that if developed correctly with the help of some strategically placed AI-powered tools, one can become quite addicted to studying pretty much anything. Think about how musicians effectvely learn complex pieces by breaking those into small chunks and then practicing them, check out a nice video on this topic here.</p> <p>Building these micro-flows is not a result of you learning tool X or Y, its mostly about learning to identify how you as a learner does things, and which tools you're comfortable with, so that you can find that perfect middle ground that balances the tradeoff between the friction of learning new tools and ways of doing things with the knowledge and tooling you already have to get you to build flows that work perfectly for you. </p> <p> Image generated with GPT-4o</p> <p>What is really powerful about using AI is that now, a walk through the park can be a study session, I for example love reading papers and long web articles while walking around my neighbourhood in the morning. A flow that really works for me here is:</p> <p></p> <p>Image generated by me! :)</p> <p>For example, for researching this article and for my workshop I spent one morning walking around and discussing this this article by \"The Guardian\", you can check out that conversation here.</p> <p>What I like about this is that by framing studying as the process of going through these focused sessions I call 'micro-flows' you learn this skill of developing more intelectual agency and ownership over the knowledge being acquired while leveraging technologies to guarantee you don't waste time doing stuff that's not helping you progress.</p> <p>Another micro-flow I like for language learning is:</p> <ol> <li>Use ChatGPT Advanced Mode on the mobile app</li> <li>I use a prompt like this: <code>You are a language tutor that speaks language {X} and {Y}, I'm learning language {X}, speak to me in this language and if I make a mistake correct me on that language unless I ask explicitly for you to explain it in language {Y}, start by asking me a random question.</code></li> <li>Chat with ChatGPT in my target language</li> <li>Repeat</li> </ol> <p>A slightly more advanced example involves asking Claude (I prefer Claude 3.7 Sonnet right now for code generation ) to generate  a quiz app that I can run locally with pure html/javascript code that allows me to load a <code>.json</code> file and upload a structured quiz into it. Check out an example chat for reference here.</p> Learning with AI Is Learning Self-Control <p>As we've discussed before learning properly with AI is directly related to your ability to self-control and contain the impulpse to outsource the wrong things to an LLM. However, that's easier said than done, given the an individual's energy levels throughout a day, week or month varies depending on a bunch of external factors, the fact that we can't necessarily have full control of these represents a significant danger in this self-regulatory battle. </p> <p>What I mean is that, as our energy levels go down, the likelihood of outsourcing the wrong things to AI goes up, meaning that   if you're tired, you have an exam coming up, you're not sleeping well, and you have to read say a 45 page manual on quantum mechanics,   you may start asking for answers more than actually engaging with the material meaningfully.</p> <p>To prevent that it's important to develop through either daily logs or some serious self-talk a powerful sense of your internal state   that allows you to know at any point in time wether or not you're progressing or if you're just fooling yourself with low quality    sessions, specially given that AI can produce this very powerful false impression of knowledge acquired through blind generation of information.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#a-tool-must-be-predictable","title":"A Tool Must Be Predictable","text":"<p>One thing I like about this idea of the micro flow is that it tackles one of the most salient flaws and weaknesses of AI tools, which is its undeterministic nature. Now by design, a tool to be useful must be predictable, meaning a person must have a very clear representation of the output it will get by using that tool. That's what gets us to use tools like hammers and keys and so on. However, due to the undeterministic nature of AI, sometimes it becomes quite challenging to understand how to use it. However, by inserting it into a micro flow where its undeterminism is put in check by having a loop of concrete outcomes, one can start to see the value of it.</p> <p> Image generated with GPT-4o</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#researching","title":"Researching","text":"<p>We will assume that all of the items discussed below about learning and studying will obviously apply on how AI tools can be leveraged for research because a big part of doing research is about learning about the world and studying new things. So the point is to understand what is the unique challenge that academic research puts forward that must be addressed on the process of integrating AI tools into its workflows.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#personal-context","title":"Personal Context","text":"<p>When discussing using AI for academic research, there's a different level of concern regarding the nature of this integration. Because while learning and studying, the risk or the stake of a mistake is very, very low because that's just a matter of addressing it as it happens and there's no consequence. In research, the risk is very high because we might produce untruthful statements about the world.</p> <p>Therefore, we have to look at this as a pipeline problem where we have to ask the question, what is the ideal pipeline in the areas where AI integration will definitely help a researcher discover, through run experiments, analyze data, inspect evidence, and so on, while keeping an eye on for the dangers of this integration.</p> <p>I got interested in this topic is because I have a lot of friends who are researchers, and I was impressed to find out how little they knew about the kind of stuff one can do leveraging AI tools on a day-to-day basis to improve one's research capabilities.</p> <p>So without continuing on discussing things subjectively, these are the pain points that I think are interesting to address when discussing AI tools for research. Number one, literature review, citations, collecting paper references, inspecting evidence, analyzing insights, managing and organizing data notes, leveraging tools like Judge Pettin-Claw for data analysis, generating semantically rich diagrams,</p> <p>Tracting data, converting data into different formats, like for example, generating table-like markdown, like for example, generating markdown-like tables, standardizing the process of reading and analyzing hundreds of papers, listening to long papers, leveraging deep research, a feature present in both chat-pt, perplexity, and Gemini, leveraging more academic research-specific tools that integrate AI in a robust way like ELICIT for generating both AI-powered research reports as well as many summaries of a field that leverage only academic research as their sources, and finally, leveraging something like projects to organize the notes for a specific experiment, for example.</p> Grok 3 Powered Search on AI Tools for Academic Research <p>Check out this little search I did with grok3 on how AI tools are used for academic research, leveraging real-time data from Twitter.</p> <p>Given that I am currently not doing academic research, instead of going into any formal discussion about how I believe AI2 should be used in research, I will just state a bunch of use cases that I think are practically helpful and useful:</p> AI Tools for Research Table Tool Everyday use-case Relevance Note Prompt Elicit Paste a research question (\u201cDoes omega-3 lower resting heart rate?\u201d) and instantly get a table of study summaries, population size, effect size, and direct links.Over 2 million users report saving \u2265 5 hours/week on systematic reviews. \ue200cite\ue202turn17view0\ue201 Source-anchored answers cut back on \u201challucination anxiety.\u201d Connected Papers Paste one seed article to get a force-directed map of closely related work; great for spotting forgotten sub-fields. Relevant for literature reviews workflows ChatGPT/Claude/Gemini/ChatPDF Drop a dense 40-page PDF and ask: \u201cSummarise the methods in two bullet points\u201d or \u201cWhere do the authors mention limitations?\u201d.  It hyperlinks each answer to the exact page. Relevant to deep analysis of multiple papers Zotero + AI plugins Classic reference manager gains GPT-based note-extraction, auto-tagging, and two-way sync with Obsidian for literature notes. Useful for managing pdfs, references, generating automatic bibliographies. Perplexity Punch in an open-ended query (\u201cLatest work on perovskite photovoltaics\u201d) and follow the inline citations to primary articles. Combines a ChatGPT-style conversation with live web + academic indexes. ChatGPT/Claude/Gemini/Perplexity Brainstorm search terms, generate Boolean strings for databases, or ask for missing seminal authors. LLM breadth + plugins (ScholarAI, etc.) give a Google-Scholar-like feel without ads. Surveys show 86 % of students already lean on an AI chatbot during study Hallucinated citations if you don\u2019t demand sources; always cross-check. ChatGPT/Claude/Gemini/Perplexity Literature Review Builder Find the most cited articles on [topic]. Summarize their key findings and provide publication details (author, journal, year). ChatGPT/Claude/Gemini/Perplexity Research Gap Identifier Analyze recent publications on [topic]. Identify research gaps or areas where further study is needed. ChatGPT/Claude/Gemini/Perplexity Methodology Explorer Suggest three commonly used research methodologies for studying [topic]. Include examples of studies that used them. ChatGPT/Claude/Gemini/Perplexity Citation Generator Generate APA-style citations for the following sources: [list of articles/books]. Provide accurate formatting for each citation. ChatGPT/Claude/Gemini/Perplexity Thesis Statement Advisor Create a clear and concise thesis statement for a research paper on [topic]. Include a brief rationale for its importance. ChatGPT/Claude/Gemini/Perplexity Academic Argument Structurer Help structure a well-argued essay on [topic]. Include a solid introduction, main points, and a conclusion in bullet points. ChatGPT/Claude/Gemini/Perplexity Hypothesis Formulator Propose a testable hypothesis for a study on [topic]. Include variables and potential methods for testing. ChatGPT/Claude/Gemini/Perplexity Data Source Finder Identify reliable data sources or datasets for research on [topic]. Include links and descriptions of their relevance. ChatGPT/Claude/Gemini/Perplexity Academic Presentation Creator Outline a 10-minute presentation on [topic]. Include a good introduction, three key points, and a conclusion. ChatGPT/Claude/Gemini/Perplexity Research Ethics Advisor Provide ethical considerations for conducting research on [topic]. Highlight all potential risks and how to mitigate them."},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#notes-on-data-extraction","title":"Notes on Data Extraction","text":"<p>Special note on modern browser level quick data extraction techniques: </p> <p>There are now a lot of tools that you can use directly on a browser where you replace a part of the URL and transform that specific page into a Nelalaam ready document. Some of my favorite ones are:</p> <ul> <li>gitingest - Transform GitHub repositories into Nelalaam-ready documents</li> <li>r.jina.ai - Convert any webpage into a Nelalaam-ready document</li> <li>arxiv-text - Extract text from arXiv PDFs into Nelalaam-ready format</li> </ul>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#notes-on-data-analysis","title":"Notes on Data Analysis","text":"<p>A simple pipeline/flow I like for doing data analysis with AI tools is:</p> <ol> <li>Brainstorm with o1,o3,o4,Claude 3.7 sonnet, Gemini 2.5 Pro etc... (big reasoning model)<ol> <li>Show data samples</li> <li>Outline objectives etc...</li> <li>Write a draft prd.md</li> <li>Review it make it tight</li> </ol> </li> <li>Send that to Cursor or Claude or ChatGPT and generate the analysis code in modular parts you can test individually</li> <li>Test the full pipeline on a small synthetic dataset that reflects the overall data that will go through the pipeline</li> <li>Evaluate behavior and done! :)</li> </ol> <p>You can also leverage Gemini directly on Google Colab which although I haven't used it that much seems quite promising.</p> <p>An example of a data analysis conversation I did with ChatGPT..</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#miscelaneous-examples-of-ai-usage-for-learning-studying-and-research","title":"Miscelaneous Examples of AI Usage for Learning, Studying and Research","text":"<ul> <li>Avoid reasoning models when you're quizzing yourself because they are too slow for the feedback loop you're looking for</li> <li>Generate questions in batches of 10 or 20 - put them into a custom app - try them out - track your score - etc....</li> <li>Custom GPTs for when you're reusing a prompt too much</li> <li>'Meta-cognitive' tip is that when facing topics you are completely unfamiliar with ask the model which questions should you be asking given the stage you're in.</li> <li>Use the Projects Feature for Chatting with Notes to chat with your notes. ChatGPT Projects, Claude Projects and NotebookLM are powerful tools that allow you to upload files in different formats and have a single place to organize, chat with those files, develop insights, explore and develop your thoughts in your notes.  </li> <li>Extracting \u201cBites\u201d from Papers &amp; Arguments: Prompt the model to distill every paper you read into 3-to-5 atomic \u201cbite\u201d claims, each tagged with the page number and a confidence score. Over time those bites become reusable Lego bricks for literature reviews, slide decks, or debate prep. You can do that easily with pdfs, .txt, .md files, youtube summaries and so on. </li> <li>YouTube &amp; Video Processing: Pipe the auto-generated transcript into ChatGPT and ask for a bullet outline keyed to timestamps, then jump straight to the 90-second clip that answers your question. This turns a 40-minute lecture into a surgical reference tool.  </li> <li>NotebookLM Summaries (Timestamps + Quotes): Drop long PDFs or transcripts into NotebookLM and request a summary where every bullet links back to the exact quote or timecode. The live backlink keeps you honest and makes fact-checking trivial.  </li> <li>NotebookLM Brainstorming Across Sources: Because NotebookLM treats multiple docs as one corpus, you can ask cross-cutting \u201cHow do papers A, B and C disagree on retrieval practice?\u201d and get a synthesized viewpoint rather than three isolated abstracts. You can so similar things with ChatGPT/Claude Projects feature.  </li> <li>NotebookLM Interactive Maps: Ask the model to auto-generate a concept map of all loaded sources; clicking a node reveals the supporting paragraph. Visual structure plus instant drill-down is perfect for taming messy research topics.  </li> <li>Mind-Maps &amp; Diagrams (Excalidraw Export): Have the LLM outline a process as a mermaid graph, the copy that and pasted into Excalidraw, then tweak the vector file by hand; you get both AI speed and human design finesse. Editable diagrams beat static images when you need to iterate for slides or handouts.  </li> <li>Memorizing Procedures via AI-Made Visuals: Feed a multistep recipe, protocol or workflow to an image-generation model and ask for an annotated comic-strip poster. The combination of spatial cues and color makes the steps stick far better than plain text.  </li> <li>\u201cSocr\u00e1tic\u201d Brainstorming over PDFs: Tell ChatGPT to play probe your knowledge of a material through reference-backed questions about each section of a paper\u2014so you actively defend your understanding instead of passively accepting a summary. It\u2019s retrieval practice and critical reading rolled into one.  </li> <li>Prompt for an AI-Based Teaching Assistant: A fixed system prompt (\u201cYou are my TA\u2014quiz me, correct me in-language, escalate difficulty\u201d) turns any chat into an ever-present tutor that adapts on the fly. Save it as a custom preset so the pedagogy is one click away.  </li> <li>Question Engineering: Before you ask for an explanation, draft the rubric: scope, depth, constraints, desired format. Well-engineered questions cut hallucinations and give you outputs that plug directly into flashcards, slides or code.  </li> <li>\u201cWhat If\u201d Loops in ChatGPT: Push a concept through a chain of counterfactuals (\u201cWhat if the learning rate were zero?\u201d \u2192 \u201cWhat if it doubled each epoch?\u201d) to expose edge cases and deepen intuition. Each loop forces you to predict before revealing the model\u2019s answer, sharpening mental models.  </li> <li>Deep Research Points: Use the LLM to draft a table of \u201cclaims, evidence needed, current evidence, remaining gap\u201d for every hypothesis in your project. This living doc steers literature searches and prevents hand-wavy conclusions.  </li> <li>Using \u201cProjects\u201d for Exam Preparation: Dump syllabus, past papers and your notes into a ChatGPT Project; the workspace remembers context so follow-up queries like \u201cGenerate a mock exam on week-4 material\u201d stay tightly scoped. It\u2019s a one-stop revision cockpit.  </li> <li>Getting Timestamps for Rapid Video Reference: A single prompt can return \u201ckey idea \u2192 timestamped link\u201d pairs (e.g., <code>https://youtu.be/abc?t=452</code>) so you can cite or revisit clips without re-scrubbing. Perfect for assembling evidence in presentations or essays. See this Chat for reference. </li> <li>Creating Study Guides from a Course Syllabus: Paste the syllabus and ask for a week-by-week guide with objectives, readings, and practice tasks; the model aligns each unit with Bloom levels so you can tick off progress systematically.  </li> <li>SuperWhisper + ChatGPT Transcription Loop: Record lectures or thinking-aloud sessions, transcribe with SuperWhisper, then feed the text to ChatGPT for instant summarization, flashcard generation, or gap spotting. This closes the capture-to-comprehension loop with almost zero friction.</li> <li>SuperWhisper + ChatGPT Writing/Thinking Loop: you can leverage superwhisper for fast realtime audio transcription and then feed that into ChatGPT to help you organize and structure your writing as well as give you feedback, check this article for more detail.</li> </ul>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#final-notes-for-effective-ai-usage","title":"Final Notes for Effective AI Usage","text":"<p>I think now I've covered quite a bit in terms of proper ways of integrating AI into learning, studying, and research. And overall, I think that poor usage of AI tools like ChatGPT, Claude, Gemini are just a function of both external and internal factors. For example, imagine a PhD student preparing for a thesis defense. That student's good use of AI will be a function of a serious of factors like presence and/or lack of appropriate supervision, the thesis deadline, lack of feedback and support, psychological nervousness, all of which can ultimately lead to poor patterns of usage of AI, like asking ChatGPT to write his/her thesis.</p> <p>Therefore, managing your energy levels and learning to self-regulate becomes one of the most important skills of the 21<sup>st</sup> century. So learn to manage your cognitive load and insert AI in the areas where you find you lack the most. So for things like unstucking yourself, brainstorming, getting you to develop your own ideas and work on your own projects.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#thoughts-on-future-of-human-ai-integration","title":"Thoughts on Future of Human-AI Integration","text":"<p>There's a lot of interesting stuff to say about what's the future of human AI integration. For example, Google recently released a very interesting article about accelerating scientific breakthroughs with an AI co-scientist. The idea being that there's an automated process of reproducing the scientific method and research methodologies through this flow engineering approach. Despite thinking that there's some merit to this approach, in my view, nothing's ever going to replace the intuition and intentional brilliance of a real human scientist.</p> <p> Gottweis et al. 2025</p> <p>On the other hand, approaches that I think will pick up some steam are approaches like the paper below, where you take a very small and focused aspect of doing research and you try to have some sort of a genetic workflow automation around it to speed up the process of the things that don't necessarily require a human to do, like this paper about automated hypothesis validation.</p> <p> Image taken from Huang et al. 2025</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#conclusion","title":"Conclusion","text":"<p>To me the trully interesting future of AI is actually smaller, not in attempting to replace researchers in any capacity but actually  as a cognitive technology, meaning technology that can truly aid human thought in progressing forward rather than backwars. The problem with LLMs as cognitive technologies is that they represent a threat as much as they represent a solution, given the corporate control that exists today over it.</p> <p>However, I do believe in a future where LLMs will play a positive role as elements of cognition  that we can manipulate much as we manipulpate a pencil today to draw up stuff on a paper when trying to solve a problem.</p> <p>I'm interested in questions of how LLMs can be thought of in terms of primitives, much like this amazing project by Daniel Miesler. The idea being that by leveraging LLM's capabilities we can actually break down the helpful procedures that a model can perform and integrate them into our lives and how we do stuff.</p> <p>I wrote this piece in preparation for a workshop I'll be teachiing next saturday in Lisbon with the same title.</p>"},{"location":"blog/2025/04/20/ai-tools-for-learning-studying-and-research/#references","title":"References","text":"<ul> <li>https://eclass.uoa.gr/modules/document/file.php/PPP242/Benjamin%20S.%20Bloom%20-%20Taxonomy%20of%20Educational%20Objectives%2C%20Handbook%201_%20Cognitive%20Domain-Addison%20Wesley%20Publishing%20Company%20%281956%29.pdf</li> <li>Kidd, Celeste, Steven T Piantadosi, and Richard N Aslin. 2012. \u201cThe Goldilocks Effect: Human Infants Allocate Attention to Visual Sequences That Are Neither Too Simple nor Too Complex.\u201d PLoS ONE 7 (5). Public Library of Science: e36399\u2013e36399. doi:https://doi.org/10.1371/journal.pone.0036399.</li> <li>Huang et al. 2025</li> <li>Thought as Technology</li> </ul>"},{"location":"courses/","title":"Courses","text":"<p>This is where I share my latest and upcoming courses on topics regarding using and building AI tools!</p>"},{"location":"courses/#a-quick-introduction-to-prompt-engineering","title":"A Quick Introduction to Prompt Engineering","text":"<p>This is a beginner-friendly course on creating effective prompts for LLMs. It covers prompt basics, key techniques like zero and few-shot prompting, and hands-on experiments with prompts using spreadsheets and Python code through jupyter notebooks.</p> <p>Sign up</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/llm-development/","title":"LLM Development","text":""},{"location":"blog/category/ai-best-practices/","title":"AI Best Practices","text":""},{"location":"blog/category/technical-guides/","title":"Technical Guides","text":""},{"location":"blog/category/building-agents/","title":"Building Agents","text":""},{"location":"blog/category/ai-tools--workflows/","title":"AI Tools &amp; Workflows","text":""},{"location":"blog/category/personal-journey--reflections/","title":"Personal Journey &amp; Reflections","text":""}]}