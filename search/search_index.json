{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sup Folks!","text":"<p>I'm an AI Engineer, and AI instructor at O'Reilly Media, I like writing about building cool tools.  A few years ago, after spending some time as a research assistant at the Champalimaud Foundation, I pivoted to industry and started working as an AI engineer, and now I help people develop AI models, tools, and all sorts of fun stuff.</p> <p>From time to time I also do some neat workshops about stuff I find interesting in AI.</p> <p>Subscribe to my Newsletter</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"posts/llm-usage-general-tips/","title":"Quick Tips on Using LLMs Effectively","text":"<p>All right, this is going to be my own attempt at compiling some fun examples of how to prompt LLM models effectively to do useful stuff.</p>"},{"location":"posts/llm-usage-general-tips/#prompt-iteratively","title":"Prompt Iteratively","text":"<p>I think it was Jeremy Howard who coined this term 'Dialog Engineering' where you build and engineer things through talking to an LLM in small steps.</p> <p>This could not be more true. Prompt atomically, for example: instead of asking a model to build a 'quiz app' maybe ask the model to:</p> <ol> <li>\"Design a basic data structure for quiz questions and answers\"</li> <li>\"Create a function to load and parse quiz questions from a JSON file\" </li> <li>\"Build a simple command-line interface to display questions and accept user input\"</li> <li>\"Add scoring logic to track correct/incorrect answers\"</li> <li>\"Implement a way to save quiz results and show final score\"</li> </ol> <p>Something like that where you prompt in small pieces and for each you supervise the result and incrementally grow into the final result you were looking for.</p> <p>This is a perfect segway to my next tip.</p>"},{"location":"posts/llm-usage-general-tips/#use-llms-as-assistants-not-as-replacements","title":"Use LLMs as assistants not as replacements","text":"<p>Don't treat whatever is generated with an AI as the final all might output. Treat everything you get from it critically, which I know can sound a bit contradictory given the nature of why we are using LLMs right? We are using it so we don't have to do the work. However, this approach can only lead to hours of mindless debugging and absolute dread. </p> <p>Instead, treat the model like the great Simon Willison puts it in this youtube video where he mentions you should treat them as \"smart interns\" that \"read through all the documentation\" and can help 24/7.</p> <p>I think you should use them as supporting like tools to support support the decisions that you're making... - Simon Willison </p>"},{"location":"posts/llm-usage-general-tips/#ask-for-multiple-options","title":"Ask for Multiple Options","text":"<p>Specially for tought questions, don't ask the models for one answer, ask for many and pick the ones that looks best.</p>"},{"location":"posts/llm-usage-general-tips/#use-it-to-explore-and-not-just-for-quick-answers","title":"Use it to Explore and Not Just for Quick Answers","text":"<p>Do side projects with these tools and explore what they can do instead of  relying on them just as a google search replacement.</p>"},{"location":"posts/llm-usage-general-tips/#explore-and-experiment","title":"Explore and Experiment","text":"<p>When working with AI tools, it's important to approach them with a spirit of exploration and experimentation rather than just using them for quick answers. Here are some key ways to do this:</p> <p>Challenge yourself to do complete projects using AI tools. As one developer put it: \"If you can afford to do a side project with these tools and like set yourself a challenge to write every line of code with these tools, I think that's a great thing you can do.\"</p>"},{"location":"posts/patterns-for-llm-usage/","title":"Patterns for Effective Usage of LLMs","text":""},{"location":"posts/patterns-for-llm-usage/#code-generation","title":"Code Generation","text":"<p>Paste code + error ask it to debug</p> <p>Copy paste from ChatGPT/Claude/Gemini/Llama3 and also paste in error + original code to get better answer.</p> <p>Automate Cleaning AI Output</p> <p>Setup a quick tool to clean up the Python code generated (I use an alias <code>clean-python</code>)</p> <p>Use Standalone Scripts with AI + uv</p> <p>Generate Python standalone scripts by using Claude+Projects with custom descriptions and the uv package manager</p> <p>Leverage Context</p> <p>For recent coding frameworks use the documentation as json/markdown files as context for something like Claude/ChatGPT projects. Show LLM how to call an API (in the prompt) then ask it to create something with that api.</p>"},{"location":"posts/patterns-for-llm-usage/#general-usage","title":"General Usage","text":"<p>Let it see your screen</p> <p>Use tools like Gemini 2.0 with streaming in realtime in Google AI Studio so the AI can see your screen to help you navigate new software and answer app specific questions in context.</p> <p>Save your AI Mistakes</p> <p>When AI makes a mistake save it for later as your own personal benchmark</p> <p>Build Micro-AI-Powered Data Transformation Pipelines</p> <p>Build app with AI that takes in data with a certain structure and outputs  desirable output, format, etc.... then make a prompt template that  produces the data into the format acceptable  by that app (done).  </p>"},{"location":"posts/patterns-for-llm-usage/#patterns-for-prompt-templates","title":"Patterns for Prompt Templates","text":"<p>Informed transformation </p> <pre><code>Given this {{ context }}. Do {{ action }} to this {{ content }}.\n</code></pre> <p>The OUTPUT ONLY Pattern</p> <p>Prime model at the very end to: </p> <p><code>(...previous context...) OUTPUT ONLY {{ desired output }}</code></p> <p>Use Meta Prompts</p> <p>Use prompts for prompts! Create a prompt that uses a model to  generate multiple prompts that address all the parts of your task.</p> <p></p> <p>For example:</p> <pre><code>I need to create a series of prompts to help me analyze customer feedback data. \nPlease generate 3 prompts for an LLM model to help me:\n\n1. Extract key themes and sentiment\n2. Identify urgent issues needing attention\n3. Generate actionable insights for product improvements\n\nFor each prompt you generate, explain its purpose and expected output format.\n\nOUTPUT ONLY the prompts and their explanations, formatted as such:\nPURPOSE: &lt;purpose of prompt&gt;\nInstruction: &lt;main instruction&gt;\nOUTPUT FORMAT: &lt;desired output format&gt;\n</code></pre> <p>You can also ask the model to break down the problem itself given some  initial goal or intention and then for each sub-task ask for a solution:</p>"},{"location":"posts/patterns-for-llm-usage/#which-models-to-use-when","title":"Which Models to Use When","text":""},{"location":"posts/patterns-for-llm-usage/#model-tiers","title":"Model Tiers","text":""},{"location":"posts/patterns-for-llm-usage/#tier-1-high-intelligence-slow-expensive","title":"Tier 1 (High Intelligence, Slow, Expensive)","text":"<ul> <li>For complex, nuanced tasks</li> <li>Examples: DeepSeek, O1.  </li> </ul>"},{"location":"posts/patterns-for-llm-usage/#tier-2-balanced","title":"Tier 2 (Balanced)","text":"<ul> <li>It is your daily driver for most tasks \u2013 code, emails, general queries</li> <li>Examples: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.3.</li> </ul>"},{"location":"posts/patterns-for-llm-usage/#tier-3-cheap-fast","title":"Tier 3 (Cheap, Fast)","text":"<ul> <li>For bulk, everyday tasks</li> <li>Examples: GPT-4o-mini, Gemini Flash, Llama 3.\u2153.2.</li> <li>Enables AI usage in \"every nook and cranny\"</li> </ul>"},{"location":"posts/patterns-for-llm-usage/#workflow-example","title":"Workflow Example","text":"<ol> <li>Use Tier 3 to process large documents quickly and cheaply</li> <li>Use Tier 2 to refine and apply structured outputs</li> <li>Use Tier 1 for final critical reasoning or complex synthesis</li> </ol>"},{"location":"posts/patterns-for-llm-usage/#_1","title":"Patterns for LLM Usage","text":""}]}