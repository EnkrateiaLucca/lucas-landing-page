{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sup Folks!","text":"<p>I'm an AI Engineer, and AI instructor at O'Reilly Media, I like writing about building cool tools.  A few years ago, after spending some time as a research assistant at the Champalimaud Foundation, I pivoted to industry and started working as an AI engineer, and now I help people develop AI models, tools, and all sorts of fun stuff.</p> <p>From time to time I also do some neat workshops about stuff I find interesting in AI.</p> <p>Subscribe to my Newsletter</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"posts/2025-02-16-blog-post-2/","title":"Using AI to Build Practice","text":"<p>So, these are a few examples on how one can use AI to do what I like to call building practice.</p> Building Practice <p>Building practice is about systematically tackling a desired skill like 3d printing, playing the piano and so on, through the process of building a system that involves a closed looop environment for improving particular aspects of that skill.</p>"},{"location":"posts/2025-02-16-blog-post-2/#fetching-data-from-documents","title":"Fetching data from documents","text":"<p>Document-based learning can be enhanced by uploading technical PDFs and instructing the model to answer questions specifically from that content, with references to exact pages. This creates a focused learning loop for understanding specific software functions or technical concepts. The model can also help locate relevant information online through a search action that can potentially save a lot of time on manual searches across multiple URLs.</p>"},{"location":"posts/2025-02-16-blog-post-2/#breakdown-pattern","title":"Breakdown pattern","text":"<p>One fundamental pattern is what I call the breakdown pattern where you feed the model your ultimate goal and ask it to break it down into smaller, more manageable tasks or projects. </p> <p>This creates a natural progression from simple to complex challenges, allowing you to build the necessary skills incrementally while maintaining an optimal balance of difficulty at each step.</p>"},{"location":"posts/2025-02-16-blog-post-2/#suggestion-pattern","title":"Suggestion pattern","text":"<p>To complement this approach, the suggestion pattern helps maintain momentum in your learning journey. When you're unsure what to tackle next, you can describe your current skill level and completed projects to the model, which can then suggest appropriate next steps. This eliminates friction in decision-making and keeps you moving forward in your skill development.</p>"},{"location":"posts/2025-02-16-blog-post-2/#real-time-assistance","title":"Real-time assistance","text":"<p>Real-time assistance when learning a new software tool for example you can use something like Google AI Studio's real-time screen-sharing feature with Gemini to navigate complex software interfaces like AutoDesk Fusion 360. </p> <p>While not perfect, the model can provide helpful guidance and suggestions for navigating complicated user interfaces. There are some cool experimental features being built on top of this like this Gemini Cursor.</p> <p>There are also other interesting approaches related to this concept and the idea of computer use like the new tool from Microsoft: OmniParser 2.</p> <p>More gemini-based apps can be found here: Awesome-Gemini-Apps.</p>"},{"location":"posts/2025-02-16-blog-post-2/#brainstorming","title":"Brainstorming","text":"<p>Brainstorming with AI becomes more effective when you carefully craft your prompts to receive specific, targeted interactive feedback. Rather than accepting vague advice from a generic conversation, you can structure the interaction to get precise, actionable information in your preferred format. For example, when practicing writing you might want the model to be more tuned to a certain style of writing like technical, or screenwriting and so on.</p>"},{"location":"posts/2025-02-16-blog-post-2/#planning-learning-goals","title":"Planning learning goals","text":"<p>AI can also assist with planning by helping organize tasks around existing commitments and limitations. This extends beyond skill development to general productivity, allowing you to optimize your learning schedule and maintain steady progress.</p> <p>You can prompt the model to breakdown a specific goal with a certain deadline into a set of tasks that should be executed within some pre-defined schedule (that could also be given by the model).</p>"},{"location":"posts/2025-02-16-blog-post-2/#maintaining-flow","title":"Maintaining flow","text":"<p>The overarching principle in using AI for skill development is maintaining flow state. It's crucial to avoid treating AI as a gimmick and instead craft specific, ultra-specialized interactions that provide exactly what you need. The goal isn't to replace your cognitive effort but to facilitate it. </p> <p>By developing small-scale closed learning loops with clear feedback mechanisms, you create an environment where AI enhances rather than substitutes for your learning process.</p> <p>This focused approach ensures that your interaction with AI directly supports your skill development while keeping you actively engaged in the learning process.</p>"},{"location":"posts/2025-03-08-blog-post-3/","title":"An Audio-based AI-co-writing workflow","text":""},{"location":"posts/2025-03-08-blog-post-3/#step-1-download-superwhisper-for-audio-transcription","title":"Step 1 - Download Superwhisper for Audio Transcription:","text":"<ul> <li>For IOS Users</li> </ul> <p>Superwhisper allows users to dictate thoughts and ideas, transforming them into structured text. This hands-free approach enables brainstorming sessions during activities like walking or commuting.  </p>"},{"location":"posts/2025-03-08-blog-post-3/#step-2-use-chatgpt-for-idea-expansion-and-structuring","title":"Step 2 - Use ChatGPT for Idea Expansion and Structuring:","text":"<ul> <li>Download the ChatGPT app on the APP Store</li> </ul> <p>ChatGPT assists in organizing, expanding, and refining transcribed ideas. With a ChatGPT Plus subscription, users gain access to advanced features, enhancing the creative writing process.</p>"},{"location":"posts/2025-03-08-blog-post-3/#step-3-integrate-transcriptions-into-chatgpt-for-refinement","title":"Step 3 - Integrate Transcriptions into ChatGPT for Refinement","text":"<ul> <li>Import the transcribed content from Superwhisper into ChatGPT.</li> <li>Use ChatGPT to brainstorm, reframe, and expand upon the initial ideas, structuring them into coherent narratives or chapters.</li> </ul>"},{"location":"posts/2025-03-08-blog-post-3/#step-4-review-drafts-during-activities","title":"Step 4 - Review Drafts During Activities","text":"<ul> <li>Convert drafts into audio formats to listen during walks or other activities.</li> <li>You can do that with apps like speechify</li> <li>Note down feedback, ideas, or areas of improvement during these sessions (you can do it all with audio using super whisper or recording on your phone using the Iphone default voice memo app or something similar)</li> </ul>"},{"location":"posts/2025-03-08-blog-post-3/#step-5-incorporate-feedback-into-revisions","title":"Step 5 - Incorporate Feedback into Revisions","text":"<ul> <li>Input the gathered feedback into ChatGPT to refine and enhance the drafts.</li> <li>Repeat this iterative process until the desired quality is achieved.</li> <li>At this stage compile insights as bites into something like a notes app or doc.</li> </ul>"},{"location":"posts/2025-03-08-blog-post-3/#step-6-finalize-and-share","title":"Step 6 - Finalize and Share","text":"<ul> <li>Once satisfied, compile the chapters or sections into a complete manuscript.</li> <li>Share the final version with peers, editors, or publish as desired.</li> </ul>"},{"location":"posts/2025-03-08-blog-post-3/#example-walkthrough","title":"Example Walkthrough","text":"<ol> <li>Open SuperWhisper on your phone (step 3)</li> <li>Dictate your thoughts about what you want to write about or expand (make these dictations small less than a minute) (step 3)</li> <li>Copy paste that into ChatGPT with a Prompt that gives it the context of what you want to do for example you can prompt ChatGPT with this: (step 3) <pre><code>I\u2019m going to brainstorm plot ideas for a space opera novel. Please take everything I say, focusing on key characters, main conflict, and potential locations. At the end, organize the notes into a simple bullet list with headings for Characters, Conflict, and Setting.\n\n{{PASTE YOUR AUDIO TRANSCRIBED FROM SUPERWHISPER HERE}}\n</code></pre></li> <li> <p>Take that output and reflect on it (perhaps during another walk?) and if you want to expand on it you can use the feature \"Advanced Voice Mode\" from ChatGPT directly on your phone to 'talk' to ChatGPT about those ideas. (step 3-4-5)</p> </li> <li> <p>Do this in a loop:</p> </li> <li>Record your thoughts with superwhisper</li> <li>Copy paste that into ChatGPT with some input for how you want to process those 'raw thoughts'<ol> <li>It can be like 'Structure this into bullet points'</li> <li>But also something like: 'Can you rephrase these ideas?'</li> <li>You can do whatever! Be creative</li> </ol> </li> <li>Take whatever output you get back as your starting point for the next part (always working in small gradual reflexive steps)</li> <li>Copy paste the main insights into something like a google doc or your notes app to keep track of insights as bites of thoughts that will be later compiled into something like a full blog post, chapter, etc...</li> <li>Iterate until you have a draft (that usually is a bunch of rich insightful notes inside ChatGPT or that you copy pasted from there into some notes app or doc)</li> </ol>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/","title":"AI Tools, Life, Travel &amp; Thoughts","text":"<p>In this post I just want to talk a little bit about some of the stuff I've been doing, what I've been 'working on', some AI tools I've been playing around with and some other stuff.</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#ai-tools-cursor","title":"AI Tools - Cursor","text":"<p>This year I've been using Cursor more and more, I've been following in love with the process of finding new avenus of leveraging LLMs in new and exciting ways. On the one hand I carry a bit of that fear of never being able to code again without some AI hanging over me, on the other, building things through conversation can be quite exciting. </p> <p>Even as I write this post I'm thinking! \"Hey, I don't have a simple automation to create a references section for my article!\". Then, immediately I already think: \"Ah, never mind, I can just ask Cursor to generate that when I'm done writing.</p> <p>Cursor is cool, and these are some of the things I've been getting more interested lately and intend to explore:</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#1-cursor-rules-files","title":"1. Cursor rules files","text":"<p>These are like files you can easily create <code>.cursorrules</code> and then inside you describe how Cursor should behave within the scope of a project.</p> <p>The cool thing is that you can make it even better by creating a folder on your root like: <code>.cursor/rules</code>, and then inside you can write spefications per programming language or file type, its bananas.</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#2-cursor-agentic-mode","title":"2. Cursor Agentic Mode","text":"<p>It's been fun to watch Cursor building whatever I want by just asking, then see the whole process unwind in front of me as if I'm some sort of low budget god of silly little apps.</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#3-mcp-cursor","title":"3. MCP + Cursor","text":"<p>I'll be honest, this is one of those that I haven't fully explored yet, essentially MCP came out and took the AI engineering world by storm. I haven't studied it in full yet, but essentialy its a standardization of what previously was simply LLM + tool calling in the wild. Now we have something more like a standard to follow to connect LLMs to tools, resources and prompts, and the promise seems to be quite exciting.</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#ai-tools-aider-claude-code-building-rust-apps","title":"AI Tools - Aider, Claude-code, Building Rust apps","text":"<p>Another fun things I've been doing lately is attempting to replace doom scrolling with what I think I'll start calling 'doom app creating' which essentially means talking to aider via its really neat <code>/voice</code> mode and essentially just ask it to build whatever is in my mind.</p> <p>What fascinates me more about this approach is how \"un-creative\" I can be, which I know, sounds kind of self-deprecating, but it does piss me off, like I am living in the most creative-friendly era in history and all I can come up with is like 'a todo-list with some fun interactive animations'? God damn...Davinci would have been very disappointed. </p> <p>Nevertheless, some times I have my moments and I think I come up with ideas that could at least have some potential, if it wasn't for me abandoning them immediately after I build a first 'barely functional' version of it. Some of the highlights for this week built with either Cursor or aider were:</p> <ol> <li> <p>Silly todo-list with rust (yes I built a todo list leave me alone)</p> <p></p> </li> <li> <p>A 'barely working' clipboard manager</p> <p></p> </li> <li> <p>A completely useless markdown note taking app (again why? It is beyond me at this point)</p> <p></p> </li> <li> <p>The worst 'Focus App' ever (essentially a timer with a dot in the middle for you to 'focus') </p> </li> <li> <p>A template generator app to leverage llms to iterate on templates for very precise/structured command generation (this one I think had some future!)</p> <p></p> <p>The idea here I think was actually interesting, essentially today it is super easy to generate like big terminal commands with LLMs, and I wanted to have like a super sleak, easy to use tool that would give me a minimal interface allowing to iterate on a certain structured command like the one in the image above. However, I tend to leave these silly little projects behind...but at least I wrote this little piece on it so it won't be completely forgotten.</p> </li> <li> <p>App to highlight code on the specific parts to edit given a prompt (again I liked this one!).     Again with this one, even though obviously you get this type of functionality in any IDE these days, I wanted something easy that works well in the terminal and again, it was just a minute with Cursor to have a perfectly functional version! Also, I am really interested in like ways to leverage AI to visualize stuff better so this was actually the first app I made with that concept in mind. </p> </li> <li> <p>App that allows you to navigate highlighted portions of a text     The idea here was that, since now it's so easy and fast to use embeddings (even in the terminal!) to select parts of a large text given some input (like a question, or a semantic request like 'the tools used in this paper'), I wanted to have a simple and easy to use interface to navigate those portions of the text once the relevant sections were extracted. Still have some faith for this project....</p> </li> </ol> <p>There are so many more but I think those are enough to pain the picture of the mess that is my 'random-apps' folder. Some of them were built with <code>aider</code> some with the new <code>claude-code</code>, all of them were built kind of without strong intentions, just as a exercise on chaotic experimentation.</p> <p>Now, one last note I'll say on this topic is the revolutionary change on my workflow and day-to-day by integrating llms in the terminal using SImon Willison's LLM-CLI. I can now write commands like:</p> <p><pre><code>`for f in *.txt; do echo ${f%.txt}.pdf &gt;&gt; file_classification.txt; cat $f | llm 'Give this file a one sentence description' &gt;&gt; file_classification.txt\n</code></pre> Which for me, has been the most fun I've had with AI besides what I'm about to tell you in the end of this article ;) .</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#ai-tools-llm-htmljs-apps","title":"AI Tools - LLM + HTML/JS Apps = \u2764\ufe0f","text":"<p>One combination I've found to work really well for a lot of stuff is to combine a powerful code model like Claude-3.7-sonnet, and then ask it to generate an app in pure html/js combined with this idea of splitting the logic of the code itself from the logic of the data (something that a lot of developers would be like 'seriously? Now you see the value of that? Oh wow!' but hey! I'm new-ish at this ok?). </p> <p>What you get is the ability to make apps that have 0 dependencies, can run directly on your browser and just works, so its like...whatever app you use that is simple enough can probably be replaced by a prompt,, 15 minutes and a cup of coffee ;).</p> <p>My biggest sucess with this combination was to write this quiz app in a single <code>.html</code> file that I use to test myself on different subjects. </p> <p>To say it has a 'simple interface' it would be an understatement, its as plain as it gets: </p> <p></p> <p>I load the questions in the right format (almost always generated by AI ;)) and then I quiz myself on whatever. The biggest use I had was to prepare for a 'The Office' themes quiz night at a bar here in Lisbon (hey we got 2<sup>nd</sup> place ok! B) ).</p> <p></p> <p></p> <p></p> <p>I've made some upgrades to this app but I haven't pulled the trigger yet because I love the simplicity of this current version.</p> <p>What I love the most about this is to be able to think of something that could be small and simple, yet powerfullly useful and effective for my current needs and workflows, and then just make it, and then use it! How cool is that?</p> <p>I even made a course that I teach live at the O'Reilly platform where I talk about this subject and I generally use this quiz app example as one of the study cases. Here is the course if you're interested:</p> <ul> <li>Building Simple Web Apps with AI Tools</li> </ul> <p>There is a lot to develop for this course still but I am excited for the next iteration of it coming up this year.</p>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#some-personal-stuff","title":"Some Personal Stuff","text":"<p>Ok, to be quick, let's list a few things that I think were good this year:</p> <ol> <li>LangChain Retweeted my article about building LLM agents in 3 levels of complexity (it got over 30.3k views!)</li> <li>I hosted my very first in-person AI Tools Workshop! This first iteration was about tools for filmmaking. I could have done a better job with the structure, but overall people really liked it and I think it was packed with actionable tips and practically insightful ideas. </li> <li>I've been working on my new workshop that I intend to publicize soon about AI tools for studying, learning and research with a focus on actually helping students and researchers to get the most out of AI without having to necessarily off load all the cognitive load and the effort of something to the AI.</li> <li>I also had my first 2 potential AI clients this year which is super exciting, both were related to building agents so that's something I'll probably be focusing this year.</li> <li>I had even more courses approved at OReilly where I currently teach a variety of online courses about LLMs, Agents and so on. The list is below if you want to check them out:<ol> <li>Using AI Tools and Python to Automate Tasks</li> <li>Building AI Apps with Gemini 2.0</li> <li>Working with o1, DeepSeek, and Gemini 2.0 Reasoning Capabilities</li> </ol> </li> </ol> <p>I also have 2 other courses right now in the works that I hope will be live in the second half of this year.</p> <ol> <li> <p>Working remote has its perks, one of which is travelling whenever I want! This year I've been to a few places already:</p> <ol> <li>Netherlands</li> <li>London</li> <li>Ireland</li> <li>Italy (Venice, Verona, Milan)</li> <li>Switzerland (spent a day in Zurich! :))</li> </ol> <p> </p> </li> </ol>"},{"location":"posts/2025-03-22-ai-tools-life-thoughts-2025/#final-thoughts-on-working-in-ai","title":"Final Thoughts on Working in AI","text":"<p>It has been a bit surreal, over these past 2 years, how my life changed completely because of AI. Like if only I knew how right my decision was to leave Brazil and come to Europe to pursue it. </p> <p>I remember it like it was yesterday, my excitement when I first started working at the Champalimaud Foundation, considered to be a world class lab for contributions to artificial intelligence I felt like that experience was going to shape my upcoming years, like my life was about to radically change, and indeed it happened! Even tough I didn't pursue a PhD in the lab, having finished my masters there and then having worked there as a hired research assistant (working on an application of generative adversarial networks to neuroscience settings) really helped me understand the inner workings of AI, how to use it, and how to get the most out of it.</p> <p>Now, working as an AI engineer, freelancer, AI instructor, everything seems quite magical, like sometimes I have to pinch myself because it feels unreal that I get paid to do the kind of stuff I get paid to do.</p> <p>From now on I'll be focusing more on producing higher technical quality content on Youtube, pursue more clients as a freelancing AI engineer, and continue to produce courses with OReilly as well as on my own, I can't wait for what's next :).</p>"},{"location":"posts/intuitive-introduction-to-langgraph/","title":"Intuitive introduction to langgraph","text":"<p>article--- title: An Intuitive Introduction to LangGraph date: 2025-02-10</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#an-intuitive-introduction-to-langgraph","title":"An Intuitive Introduction to LangGraph","text":""},{"location":"posts/intuitive-introduction-to-langgraph/#routing-stuff","title":"Routing Stuff","text":"<p>I'm working on my LangGraph course and I kind of wanted to do a little recap of how I see this framework in the big scheme of things.</p> <p>When working with LLMs, one common pattern that exists is to route stuff, meaning you send some input to an LLM and the output of that gets routed to either another LLM as described in this article by Anthropic about common patterns for working with LLMs or even to some other functionality like you might use some Python function to clean the output of that LLM.  </p> <p></p> <p>This is great because the LLM has some type of understanding of what is going on even though we still don't really understand the nature of this understanding, however it is good enough that we can actually use it to make some processing pipeline a bit leaner by having LLMs make certain decisions within some narrow scopes of a workflow.</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#the-abstraction-challenge","title":"The Abstraction Challenge","text":"<p>Now, if I am writing up a system that can perform complex tasks, </p> What do I mean by complex? <p>Integration of a diverse set of components that do all sorts of different things like performing actions, calling apis, processing massive amounts of diverse data like text and images, and so on.</p> <p>it is a bit problematic to implement it just as simple calls to apis like OpenAI or Anthropic's, because we want that system to be pragmatically improvable (robust, consistent). </p> <p>What I mean is that despite wanting the flexibility brought by LLMs, we also want the controllability offered by writing software that does stuff deterministically and can be systematically improved over time.</p> <p>So that begs the question of how we create useful abstractions around the capabilities of Large Language Models? </p> <p>To understand that, we need to understand what we are trying to abstract, for that, let's take the most common pattern in this emerging field of Agents that is growing quite a bit this year: React Agent.</p> <p></p> <p>What is there to abstract?</p> <p>Well, a bunch of stuff, if want to be able to develop a system aroud these things we need to be able to abtract things like:</p> <ol> <li>Messaging between user and LLM apis</li> <li>Calling LLM apis</li> <li>Integrating tools into LLMs as functions that call external APIs</li> </ol> <p>and much more (see diagram below).</p> <p></p> <p>Not only that, but we also want to be able to track and monitor these parts so that when problems occured we can investigate what happen and debug our system across all of its parts.</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#langchain","title":"LangChain","text":"<p>LangChain came into the scene as a framework that allowed you to put all these different parts that are common when building LLM apps, into the same 'system'. </p> <p>In LangChain everything is a runnable, which means, things like calling an LLM, calling a tool that performs some action, the prompt that you send to the model, and many other things like that become all components in this runnable interface that can be organized via the usage of a declarative language called LCEL (LangChain Expression Language).</p> <p>The idea is that you can create <code>chains</code> which in LangChain are building blocks made out of the component parts like <code>prompt template</code> <code>chat models</code> and many others, in order to create modular workflows that have swappable parts.</p> <p></p> <p>LangChain became extremely popular I think in part because they were the first to realize that there was much more value to be extracted from LLMs than just asking them for text and getting results back, building on top of super important papers that started to explore these additional functionalities like:</p> <ul> <li>Toolformer: Language Models Can Teach Themselves to Use Tools</li> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</li> <li>ReAct: Synergizing Reasoning and Acting in Language Models</li> </ul> <p>However, building this framework came at the cost of having to make choices for how the infrastructure around LLMs should look like, which is not a trivial problem because we are talking about a technology in its infancy, consider that ChatGPT was released in 2022!</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#langgraph","title":"LangGraph","text":"<p>Regardless of what the ultimate abstraction for LLMs look like, what we know right now is that there are common patterns for using it, which usually involve taking some input and passing it through a set of transformations, some of which are handled by LLMs (so not completely deterministic) and some are handled by normal code.</p> <p>What the LangChain folks realized is that most of what we do with LLMs could actually be handled by treating the entire process as graphs with nodes and edges to integrate controllability into the system while mixing it with the flexible nature of LLMs.</p> <p>So LangGraph was born.</p> <p>The way I look at it, LangChain gives you the material to do the basics with LLMs like:</p> <ul> <li>Call LLM APIs</li> <li>Integrate Prompts</li> <li>Loading documents</li> <li>Tools, retrievers for rag systems</li> <li>....</li> </ul> <p>however, when it comes to putting together a system, connecting different things and managing all of that, the LCEL comes short of something simple and intuitive that can give you manageable complexity and controlability.</p> <p>So LangGraph shows up as a framework that can take in the standardization provided by operating on LangChain Components, and provide the graph building capabilities that are considerably more intuitive when compared to 'chaining runnables'.</p> <p></p> <p>In LangGraph you introduce cycles into these chains, in the form of 'controlled flows' or 'state machines':</p> <p>LangGraph is a way to create these state machines by specifying them as graphs.</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#what-do-we-get-with-graphs","title":"What Do we Get with Graphs?","text":""},{"location":"posts/intuitive-introduction-to-langgraph/#states-nodes-edges","title":"States, Nodes &amp; Edges","text":"<p>In LangGraph the logic is to define some initial 'state' which is a data structure that will be updated throughout the execution of the graph.</p> <p>Let's take this diagram we showed of the simple react agent loop, where we have some input coming in to a model connected to some tools, and the model is going to go on a loop of calling said tools until a response is generated.</p> <p>For something like that we would need to define:</p> <ol> <li>The LLM to use</li> <li>The tools that model has access to</li> <li>Connect LLM to the tools (so the model can create the arguments for it)</li> </ol> <pre><code>from langchain_community.tools import TavilySearchResults \nfrom langchain_anthropic import ChatAnthropic\n\n# The LLM\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n# The tool (for web search)\nsearch_tool = TavilySearchResults()\n# Connect LLM to tool \nllm_with_tools = llm.bind_tools([search_tool])\n</code></pre> <p>Now, we need to take this core set of functionality we've built and inserted in a graph. To do so we need:</p> <ol> <li>To define a state</li> <li>Nodes (that perform the computations)</li> <li>Edges (that connect everything together)</li> </ol> <p>The state will define this updatable data structure that will change throughout the execution of the graph. The nodes will be the functions that perform the computations that happen throughout this graph like running LLM calls, using the tools and so on.</p> <p>The edges will join all the nodes together defining the direction of this graph.</p> <p>We start by defining the nodes as simple python functions:</p> <pre><code># The node where the LLM+tools is called\ndef llm_node(state: MessagesState):\n    # Gets the latest message in the messages list\n    response = llm_with_tools.invoke(state[\"messages\"])\n    print(\"response\")\n    print(response)\n    # returns a dictionary that corresponds to updating the state\n    # adding a the message from the model's response\n    return {\"messages\": [response]}\n\n# The node that performs the conditional logic that defines whether the\n# LLM will call a tool and return a final output to the user \ndef router_node(state: MessagesState):\n    if state[\"messages\"][-1].tool_calls:\n        # routes to a node called: \"tools\"\n        return \"tools\"\n\n    # if there is no action required, we end the loop\n    return END\n</code></pre> <p>NOw that we have the nodes as python functions, we can define our graph.  We start by defining the initial state as a <code>MessagesState</code> object, which means its a data structure that contains a default list of messages inside + the ability to add more messages to this list as the graph get's executed.</p> <pre><code>builder = StateGraph(MessagesState)\n</code></pre> <p>We integrate the nodes we've created earlier: <pre><code>builder.add_node(\"llm\", llm_node)\nbuilder.add_node(\"tools\", tool_node)\n</code></pre></p> <p>We set an entry point for the graph <pre><code># we set the entry point for the graph to be the llm\n# which means the user will send the input directly to the LLM\nbuilder.add_edge(START, \"llm\")\n</code></pre> Connect the llm to the function containing the conditional logic that routes information either to the user (END) or to the node containing the tools, and compile the graph.</p> <pre><code>builder.add_conditional_edges(\"llm\", router_node, [\"tools\", END])\n# Here we make it so that the output of the tools node has to go to the \"llm\"\nbuilder.add_edge(\"tools\", \"llm\")\n\ngraph = builder.compile()\n</code></pre> <p>Great! Now we can see how it looks!</p> <p><pre><code>try:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    print(\"Did not display graph\")\n    pass\n</code></pre> </p> <p>Looks pretty simple right? But now, we can actually have it look up information online and return written results!</p> <pre><code>graph.invoke(\n    {\n        \"messages\": [\"human\", \"Search online for current AI conferences happening in Silicon Valley right now.\"]\n    }\n)\n</code></pre> <p>There are many other core concepts in LangGraph such as:</p> <ul> <li>Persistence &amp; Memory</li> <li>Subgraphs</li> <li>Configuration</li> <li>Command</li> <li>....</li> </ul> <p>We'll leave that for another article! :)</p> <p>Cheers!</p> <p>Subscribe to my Newsletter</p>"},{"location":"posts/llm-usage-general-tips/","title":"Quick Tips on Using LLMs Effectively","text":"<p>All right, this is going to be my own attempt at compiling some fun examples of how to prompt LLM models effectively to do useful stuff.</p>"},{"location":"posts/llm-usage-general-tips/#prompt-iteratively","title":"Prompt Iteratively","text":"<p>I think it was Jeremy Howard who coined this term 'Dialog Engineering' where you build and engineer things through talking to an LLM in small steps.</p> <p>This could not be more true. Prompt atomically, for example: instead of asking a model to build a 'quiz app' maybe ask the model to:</p> <ol> <li>\"Design a basic data structure for quiz questions and answers\"</li> <li>\"Create a function to load and parse quiz questions from a JSON file\" </li> <li>\"Build a simple command-line interface to display questions and accept user input\"</li> <li>\"Add scoring logic to track correct/incorrect answers\"</li> <li>\"Implement a way to save quiz results and show final score\"</li> </ol> <p>Something like that where you prompt in small pieces and for each you supervise the result and incrementally grow into the final result you were looking for.</p> <p>This is a perfect segway to my next tip.</p>"},{"location":"posts/llm-usage-general-tips/#use-llms-as-assistants-not-as-replacements","title":"Use LLMs as assistants not as replacements","text":"<p>Don't treat whatever is generated with an AI as the final all might output. Treat everything you get from it critically, which I know can sound a bit contradictory given the nature of why we are using LLMs right? We are using it so we don't have to do the work. However, this approach can only lead to hours of mindless debugging and absolute dread. </p> <p>Instead, treat the model like the great Simon Willison puts it in this youtube video where he mentions you should treat them as \"smart interns\" that \"read through all the documentation\" and can help 24/7.</p> <p>I think you should use them as supporting like tools to support support the decisions that you're making... - Simon Willison </p>"},{"location":"posts/llm-usage-general-tips/#ask-for-multiple-options","title":"Ask for Multiple Options","text":"<p>Specially for tought questions, don't ask the models for one answer, ask for many and pick the ones that looks best.</p>"},{"location":"posts/llm-usage-general-tips/#use-it-to-explore-and-not-just-for-quick-answers","title":"Use it to Explore and Not Just for Quick Answers","text":"<p>Do side projects with these tools and explore what they can do instead of  relying on them just as a google search replacement.</p>"},{"location":"posts/llm-usage-general-tips/#explore-and-experiment","title":"Explore and Experiment","text":"<p>When working with AI tools, it's important to approach them with a spirit of exploration and experimentation rather than just using them for quick answers. Here are some key ways to do this:</p> <p>Challenge yourself to do complete projects using AI tools. As one developer put it: \"If you can afford to do a side project with these tools and like set yourself a challenge to write every line of code with these tools, I think that's a great thing you can do.\"</p>"},{"location":"posts/patterns-for-llm-usage/","title":"Patterns for Effective Usage of LLMs","text":""},{"location":"posts/patterns-for-llm-usage/#code-generation","title":"Code Generation","text":"<p>Paste code + error ask it to debug</p> <p>Copy paste from ChatGPT/Claude/Gemini/Llama3 and also paste in error + original code to get better answer.</p> <p>Automate Cleaning AI Output</p> <p>Setup a quick tool to clean up the Python code generated (I use an alias <code>clean-python</code>)</p> <p>Use Standalone Scripts with AI + uv</p> <p>Generate Python standalone scripts by using Claude+Projects with custom descriptions and the uv package manager</p> <p>Leverage Context</p> <p>For recent coding frameworks use the documentation as json/markdown files as context for something like Claude/ChatGPT projects. Show LLM how to call an API (in the prompt) then ask it to create something with that api.</p>"},{"location":"posts/patterns-for-llm-usage/#general-usage","title":"General Usage","text":"<p>Let it see your screen</p> <p>Use tools like Gemini 2.0 with streaming in realtime in Google AI Studio so the AI can see your screen to help you navigate new software and answer app specific questions in context.</p> <p>Save your AI Mistakes</p> <p>When AI makes a mistake save it for later as your own personal benchmark</p> <p>Build Micro-AI-Powered Data Transformation Pipelines</p> <p>Build app with AI that takes in data with a certain structure and outputs  desirable output, format, etc.... then make a prompt template that  produces the data into the format acceptable  by that app (done).  </p> <p>Conceptual Knowledge Prompting</p> <p>When creating knowledge management prompts (like for Anki cards), structure them around these 5 key dimensions: - Attributes &amp; tendencies - Similarities and differences - Parts &amp; Wholes - Causes &amp; Effects - Significance &amp; Implications</p> <p>This helps move beyond simple memorization towards deeper encoding of knowledge.</p> <p>Background AI Assistance</p> <p>Run lighter models (Tier 3) in the background to provide gentle guidance without disrupting workflow. For example: - Watching note creation to suggest knowledge structuring patterns - Using SuperWhisper to transform free-form thinking into structured content - Converting audio brainstorming into written documentation</p>"},{"location":"posts/patterns-for-llm-usage/#patterns-for-prompt-templates","title":"Patterns for Prompt Templates","text":"<p>Informed transformation </p> <pre><code>Given this {{ context }}. Do {{ action }} to this {{ content }}.\n</code></pre> <p>The OUTPUT ONLY Pattern</p> <p>Prime model at the very end to: </p> <p><code>(...previous context...) OUTPUT ONLY {{ desired output }}</code></p> <p>Use Meta Prompts</p> <p>Use prompts for prompts! Create a prompt that uses a model to  generate multiple prompts that address all the parts of your task.</p> <p></p> <p>For example:</p> <pre><code>I need to create a series of prompts to help me analyze customer feedback data. \nPlease generate 3 prompts for an LLM model to help me:\n\n1. Extract key themes and sentiment\n2. Identify urgent issues needing attention\n3. Generate actionable insights for product improvements\n\nFor each prompt you generate, explain its purpose and expected output format.\n\nOUTPUT ONLY the prompts and their explanations, formatted as such:\nPURPOSE: &lt;purpose of prompt&gt;\nInstruction: &lt;main instruction&gt;\nOUTPUT FORMAT: &lt;desired output format&gt;\n</code></pre> <p>You can also ask the model to break down the problem itself given some  initial goal or intention and then for each sub-task ask for a solution:</p>"},{"location":"posts/patterns-for-llm-usage/#which-models-to-use-when","title":"Which Models to Use When","text":""},{"location":"posts/patterns-for-llm-usage/#model-tiers","title":"Model Tiers","text":""},{"location":"posts/patterns-for-llm-usage/#tier-1-high-intelligence-slow-expensive","title":"Tier 1 (High Intelligence, Slow, Expensive)","text":"<ul> <li>For complex, nuanced tasks</li> <li>Examples: DeepSeek, O1.  </li> </ul>"},{"location":"posts/patterns-for-llm-usage/#tier-2-balanced","title":"Tier 2 (Balanced)","text":"<ul> <li>It is your daily driver for most tasks \u2013 code, emails, general queries</li> <li>Examples: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.3.</li> </ul>"},{"location":"posts/patterns-for-llm-usage/#tier-3-cheap-fast","title":"Tier 3 (Cheap, Fast)","text":"<ul> <li>For bulk, everyday tasks</li> <li>Examples: GPT-4o-mini, Gemini Flash, Llama 3.\u2153.2.</li> <li>Enables AI usage in \"every nook and cranny\"</li> </ul>"},{"location":"posts/patterns-for-llm-usage/#workflow-example","title":"Workflow Example","text":"<ol> <li>Use Tier 3 to process large documents quickly and cheaply</li> <li>Use Tier 2 to refine and apply structured outputs</li> <li>Use Tier 1 for final critical reasoning or complex synthesis</li> </ol>"}]}