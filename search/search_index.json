{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Sup Folks!","text":"<p>I'm an AI Engineer, and AI instructor at O'Reilly Media, I like writing about building cool tools.  A few years ago, after spending some time as a research assistant at the Champalimaud Foundation, I pivoted to industry and started working as an AI engineer, and now I help people develop AI models, tools, and all sorts of fun stuff.</p> <p>From time to time I also do some neat workshops about stuff I find interesting in AI.</p> <p>Subscribe to my Newsletter</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"posts/intuitive-introduction-to-langgraph/","title":"Intuitive introduction to langgraph","text":"<p>article--- title: An Intuitive Introduction to LangGraph date: 2025-02-10</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#an-intuitive-introduction-to-langgraph","title":"An Intuitive Introduction to LangGraph","text":""},{"location":"posts/intuitive-introduction-to-langgraph/#routing-stuff","title":"Routing Stuff","text":"<p>I'm working on my LangGraph course and I kind of wanted to do a little recap of how I see this framework in the big scheme of things.</p> <p>When working with LLMs, one common pattern that exists is to route stuff, meaning you send some input to an LLM and the output of that gets routed to either another LLM as described in this article by Anthropic about common patterns for working with LLMs or even to some other functionality like you might use some Python function to clean the output of that LLM.  </p> <p></p> <p>This is great because the LLM has some type of understanding of what is going on even though we still don't really understand the nature of this understanding, however it is good enough that we can actually use it to make some processing pipeline a bit leaner by having LLMs make certain decisions within some narrow scopes of a workflow.</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#the-abstraction-challenge","title":"The Abstraction Challenge","text":"<p>Now, if I am writing up a system that can perform complex tasks, </p> What do I mean by complex? <p>Integration of a diverse set of components that do all sorts of different things like performing actions, calling apis, processing massive amounts of diverse data like text and images, and so on.</p> <p>it is a bit problematic to implement it just as simple calls to apis like OpenAI or Anthropic's, because we want that system to be pragmatically improvable (robust, consistent). </p> <p>What I mean is that despite wanting the flexibility brought by LLMs, we also want the controllability offered by writing software that does stuff deterministically and can be systematically improved over time.</p> <p>So that begs the question of how we create useful abstractions around the capabilities of Large Language Models? </p> <p>To understand that, we need to understand what we are trying to abstract, for that, let's take the most common pattern in this emerging field of Agents that is growing quite a bit this year: React Agent.</p> <p></p> <p>What is there to abstract?</p> <p>Well, a bunch of stuff, if want to be able to develop a system aroud these things we need to be able to abtract things like:</p> <ol> <li>Messaging between user and LLM apis</li> <li>Calling LLM apis</li> <li>Integrating tools into LLMs as functions that call external APIs</li> </ol> <p>and much more (see diagram below).</p> <p></p> <p>Not only that, but we also want to be able to track and monitor these parts so that when problems occured we can investigate what happen and debug our system across all of its parts.</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#langchain","title":"LangChain","text":"<p>LangChain came into the scene as a framework that allowed you to put all these different parts that are common when building LLM apps, into the same 'system'. </p> <p>In LangChain everything is a runnable, which means, things like calling an LLM, calling a tool that performs some action, the prompt that you send to the model, and many other things like that become all components in this runnable interface that can be organized via the usage of a declarative language called LCEL (LangChain Expression Language).</p> <p>The idea is that you can create <code>chains</code> which in LangChain are building blocks made out of the component parts like <code>prompt template</code> <code>chat models</code> and many others, in order to create modular workflows that have swappable parts.</p> <p></p> <p>LangChain became extremely popular I think in part because they were the first to realize that there was much more value to be extracted from LLMs than just asking them for text and getting results back, building on top of super important papers that started to explore these additional functionalities like:</p> <ul> <li>Toolformer: Language Models Can Teach Themselves to Use Tools</li> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</li> <li>ReAct: Synergizing Reasoning and Acting in Language Models</li> </ul> <p>However, building this framework came at the cost of having to make choices for how the infrastructure around LLMs should look like, which is not a trivial problem because we are talking about a technology in its infancy, consider that ChatGPT was released in 2022!</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#langgraph","title":"LangGraph","text":"<p>Regardless of what the ultimate abstraction for LLMs look like, what we know right now is that there are common patterns for using it, which usually involve taking some input and passing it through a set of transformations, some of which are handled by LLMs (so not completely deterministic) and some are handled by normal code.</p> <p>What the LangChain folks realized is that most of what we do with LLMs could actually be handled by treating the entire process as graphs with nodes and edges to integrate controllability into the system while mixing it with the flexible nature of LLMs.</p> <p>So LangGraph was born.</p> <p>The way I look at it, LangChain gives you the material to do the basics with LLMs like:</p> <ul> <li>Call LLM APIs</li> <li>Integrate Prompts</li> <li>Loading documents</li> <li>Tools, retrievers for rag systems</li> <li>....</li> </ul> <p>however, when it comes to putting together a system, connecting different things and managing all of that, the LCEL comes short of something simple and intuitive that can give you manageable complexity and controlability.</p> <p>So LangGraph shows up as a framework that can take in the standardization provided by operating on LangChain Components, and provide the graph building capabilities that are considerably more intuitive when compared to 'chaining runnables'.</p> <p></p> <p>In LangGraph you introduce cycles into these chains, in the form of 'controlled flows' or 'state machines':</p> <p>LangGraph is a way to create these state machines by specifying them as graphs.</p>"},{"location":"posts/intuitive-introduction-to-langgraph/#what-do-we-get-with-graphs","title":"What Do we Get with Graphs?","text":""},{"location":"posts/intuitive-introduction-to-langgraph/#states-nodes-edges","title":"States, Nodes &amp; Edges","text":"<p>In LangGraph the logic is to define some initial 'state' which is a data structure that will be updated throughout the execution of the graph.</p> <p>Let's take this diagram we showed of the simple react agent loop, where we have some input coming in to a model connected to some tools, and the model is going to go on a loop of calling said tools until a response is generated.</p> <p>For something like that we would need to define:</p> <ol> <li>The LLM to use</li> <li>The tools that model has access to</li> <li>Connect LLM to the tools (so the model can create the arguments for it)</li> </ol> <pre><code>from langchain_community.tools import TavilySearchResults \nfrom langchain_anthropic import ChatAnthropic\n\n# The LLM\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n# The tool (for web search)\nsearch_tool = TavilySearchResults()\n# Connect LLM to tool \nllm_with_tools = llm.bind_tools([search_tool])\n</code></pre> <p>Now, we need to take this core set of functionality we've built and inserted in a graph. To do so we need:</p> <ol> <li>To define a state</li> <li>Nodes (that perform the computations)</li> <li>Edges (that connect everything together)</li> </ol> <p>The state will define this updatable data structure that will change throughout the execution of the graph. The nodes will be the functions that perform the computations that happen throughout this graph like running LLM calls, using the tools and so on.</p> <p>The edges will join all the nodes together defining the direction of this graph.</p> <p>We start by defining the nodes as simple python functions:</p> <pre><code># The node where the LLM+tools is called\ndef llm_node(state: MessagesState):\n    # Gets the latest message in the messages list\n    response = llm_with_tools.invoke(state[\"messages\"])\n    print(\"response\")\n    print(response)\n    # returns a dictionary that corresponds to updating the state\n    # adding a the message from the model's response\n    return {\"messages\": [response]}\n\n# The node that performs the conditional logic that defines whether the\n# LLM will call a tool and return a final output to the user \ndef router_node(state: MessagesState):\n    if state[\"messages\"][-1].tool_calls:\n        # routes to a node called: \"tools\"\n        return \"tools\"\n\n    # if there is no action required, we end the loop\n    return END\n</code></pre> <p>NOw that we have the nodes as python functions, we can define our graph.  We start by defining the initial state as a <code>MessagesState</code> object, which means its a data structure that contains a default list of messages inside + the ability to add more messages to this list as the graph get's executed.</p> <pre><code>builder = StateGraph(MessagesState)\n</code></pre> <p>We integrate the nodes we've created earlier: <pre><code>builder.add_node(\"llm\", llm_node)\nbuilder.add_node(\"tools\", tool_node)\n</code></pre></p> <p>We set an entry point for the graph <pre><code># we set the entry point for the graph to be the llm\n# which means the user will send the input directly to the LLM\nbuilder.add_edge(START, \"llm\")\n</code></pre> Connect the llm to the function containing the conditional logic that routes information either to the user (END) or to the node containing the tools, and compile the graph.</p> <pre><code>builder.add_conditional_edges(\"llm\", router_node, [\"tools\", END])\n# Here we make it so that the output of the tools node has to go to the \"llm\"\nbuilder.add_edge(\"tools\", \"llm\")\n\ngraph = builder.compile()\n</code></pre> <p>Great! Now we can see how it looks!</p> <p><pre><code>try:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    print(\"Did not display graph\")\n    pass\n</code></pre> </p> <p>Looks pretty simple right? But now, we can actually have it look up information online and return written results!</p> <pre><code>graph.invoke(\n    {\n        \"messages\": [\"human\", \"Search online for current AI conferences happening in Silicon Valley right now.\"]\n    }\n)\n</code></pre> <p>There are many other core concepts in LangGraph such as:</p> <ul> <li>Persistence &amp; Memory</li> <li>Subgraphs</li> <li>Configuration</li> <li>Command</li> <li>....</li> </ul> <p>We'll leave that for another article! :)</p> <p>Cheers!</p> <p>Subscribe to my Newsletter</p>"},{"location":"posts/llm-usage-general-tips/","title":"Quick Tips on Using LLMs Effectively","text":"<p>All right, this is going to be my own attempt at compiling some fun examples of how to prompt LLM models effectively to do useful stuff.</p>"},{"location":"posts/llm-usage-general-tips/#prompt-iteratively","title":"Prompt Iteratively","text":"<p>I think it was Jeremy Howard who coined this term 'Dialog Engineering' where you build and engineer things through talking to an LLM in small steps.</p> <p>This could not be more true. Prompt atomically, for example: instead of asking a model to build a 'quiz app' maybe ask the model to:</p> <ol> <li>\"Design a basic data structure for quiz questions and answers\"</li> <li>\"Create a function to load and parse quiz questions from a JSON file\" </li> <li>\"Build a simple command-line interface to display questions and accept user input\"</li> <li>\"Add scoring logic to track correct/incorrect answers\"</li> <li>\"Implement a way to save quiz results and show final score\"</li> </ol> <p>Something like that where you prompt in small pieces and for each you supervise the result and incrementally grow into the final result you were looking for.</p> <p>This is a perfect segway to my next tip.</p>"},{"location":"posts/llm-usage-general-tips/#use-llms-as-assistants-not-as-replacements","title":"Use LLMs as assistants not as replacements","text":"<p>Don't treat whatever is generated with an AI as the final all might output. Treat everything you get from it critically, which I know can sound a bit contradictory given the nature of why we are using LLMs right? We are using it so we don't have to do the work. However, this approach can only lead to hours of mindless debugging and absolute dread. </p> <p>Instead, treat the model like the great Simon Willison puts it in this youtube video where he mentions you should treat them as \"smart interns\" that \"read through all the documentation\" and can help 24/7.</p> <p>I think you should use them as supporting like tools to support support the decisions that you're making... - Simon Willison </p>"},{"location":"posts/llm-usage-general-tips/#ask-for-multiple-options","title":"Ask for Multiple Options","text":"<p>Specially for tought questions, don't ask the models for one answer, ask for many and pick the ones that looks best.</p>"},{"location":"posts/llm-usage-general-tips/#use-it-to-explore-and-not-just-for-quick-answers","title":"Use it to Explore and Not Just for Quick Answers","text":"<p>Do side projects with these tools and explore what they can do instead of  relying on them just as a google search replacement.</p>"},{"location":"posts/llm-usage-general-tips/#explore-and-experiment","title":"Explore and Experiment","text":"<p>When working with AI tools, it's important to approach them with a spirit of exploration and experimentation rather than just using them for quick answers. Here are some key ways to do this:</p> <p>Challenge yourself to do complete projects using AI tools. As one developer put it: \"If you can afford to do a side project with these tools and like set yourself a challenge to write every line of code with these tools, I think that's a great thing you can do.\"</p>"},{"location":"posts/patterns-for-llm-usage/","title":"Patterns for Effective Usage of LLMs","text":""},{"location":"posts/patterns-for-llm-usage/#code-generation","title":"Code Generation","text":"<p>Paste code + error ask it to debug</p> <p>Copy paste from ChatGPT/Claude/Gemini/Llama3 and also paste in error + original code to get better answer.</p> <p>Automate Cleaning AI Output</p> <p>Setup a quick tool to clean up the Python code generated (I use an alias <code>clean-python</code>)</p> <p>Use Standalone Scripts with AI + uv</p> <p>Generate Python standalone scripts by using Claude+Projects with custom descriptions and the uv package manager</p> <p>Leverage Context</p> <p>For recent coding frameworks use the documentation as json/markdown files as context for something like Claude/ChatGPT projects. Show LLM how to call an API (in the prompt) then ask it to create something with that api.</p>"},{"location":"posts/patterns-for-llm-usage/#general-usage","title":"General Usage","text":"<p>Let it see your screen</p> <p>Use tools like Gemini 2.0 with streaming in realtime in Google AI Studio so the AI can see your screen to help you navigate new software and answer app specific questions in context.</p> <p>Save your AI Mistakes</p> <p>When AI makes a mistake save it for later as your own personal benchmark</p> <p>Build Micro-AI-Powered Data Transformation Pipelines</p> <p>Build app with AI that takes in data with a certain structure and outputs  desirable output, format, etc.... then make a prompt template that  produces the data into the format acceptable  by that app (done).  </p>"},{"location":"posts/patterns-for-llm-usage/#patterns-for-prompt-templates","title":"Patterns for Prompt Templates","text":"<p>Informed transformation </p> <pre><code>Given this {{ context }}. Do {{ action }} to this {{ content }}.\n</code></pre> <p>The OUTPUT ONLY Pattern</p> <p>Prime model at the very end to: </p> <p><code>(...previous context...) OUTPUT ONLY {{ desired output }}</code></p> <p>Use Meta Prompts</p> <p>Use prompts for prompts! Create a prompt that uses a model to  generate multiple prompts that address all the parts of your task.</p> <p></p> <p>For example:</p> <pre><code>I need to create a series of prompts to help me analyze customer feedback data. \nPlease generate 3 prompts for an LLM model to help me:\n\n1. Extract key themes and sentiment\n2. Identify urgent issues needing attention\n3. Generate actionable insights for product improvements\n\nFor each prompt you generate, explain its purpose and expected output format.\n\nOUTPUT ONLY the prompts and their explanations, formatted as such:\nPURPOSE: &lt;purpose of prompt&gt;\nInstruction: &lt;main instruction&gt;\nOUTPUT FORMAT: &lt;desired output format&gt;\n</code></pre> <p>You can also ask the model to break down the problem itself given some  initial goal or intention and then for each sub-task ask for a solution:</p>"},{"location":"posts/patterns-for-llm-usage/#which-models-to-use-when","title":"Which Models to Use When","text":""},{"location":"posts/patterns-for-llm-usage/#model-tiers","title":"Model Tiers","text":""},{"location":"posts/patterns-for-llm-usage/#tier-1-high-intelligence-slow-expensive","title":"Tier 1 (High Intelligence, Slow, Expensive)","text":"<ul> <li>For complex, nuanced tasks</li> <li>Examples: DeepSeek, O1.  </li> </ul>"},{"location":"posts/patterns-for-llm-usage/#tier-2-balanced","title":"Tier 2 (Balanced)","text":"<ul> <li>It is your daily driver for most tasks \u2013 code, emails, general queries</li> <li>Examples: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro, Llama 3.3.</li> </ul>"},{"location":"posts/patterns-for-llm-usage/#tier-3-cheap-fast","title":"Tier 3 (Cheap, Fast)","text":"<ul> <li>For bulk, everyday tasks</li> <li>Examples: GPT-4o-mini, Gemini Flash, Llama 3.\u2153.2.</li> <li>Enables AI usage in \"every nook and cranny\"</li> </ul>"},{"location":"posts/patterns-for-llm-usage/#workflow-example","title":"Workflow Example","text":"<ol> <li>Use Tier 3 to process large documents quickly and cheaply</li> <li>Use Tier 2 to refine and apply structured outputs</li> <li>Use Tier 1 for final critical reasoning or complex synthesis</li> </ol>"}]}